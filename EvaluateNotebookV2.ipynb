{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Installing dependencies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-score\n",
    "# !pip install lexical-diversity\n",
    "# !pip install nltk\n",
    "# !pip install openai\n",
    "# !pip install openpipe\n",
    "# !pip install seaborn\n",
    "# !pip install sentence-transformers\n",
    "# !pip install spacy\n",
    "# !pip install transformers\n",
    "# !pip install vaderSentiment\n",
    "# !pip install tqdm\n",
    "# !pip install ollama\n",
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Bulding the metrics and calculating them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP and ML libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.util import ngrams\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bert_score import score, BERTScorer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from lexical_diversity import lex_div as ld\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# OpenAI and OpenPipe\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from openpipe import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function for each evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Multiple metrics have been tested other than what we use. For testing purposes there are some extra metrics still there. Feel free to include them in the calculation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model for syntactic similarity\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# BERTScore Calculation\n",
    "def bert_score(reference, candidate):\n",
    "    scorer = BERTScorer(model_type='bert-base-uncased')#, clean_up_tokenization_spaces=True)\n",
    "    P, R, F1 = scorer.score([candidate], [reference])\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "\n",
    "# Syntactic similarity\n",
    "def syntactic_similarity(reference, candidate):\n",
    "    ref_doc = nlp(reference)\n",
    "    cand_doc = nlp(candidate)\n",
    "    ref_deps = [token.dep_ for token in ref_doc]\n",
    "    cand_deps = [token.dep_ for token in cand_doc]\n",
    "    matcher = SequenceMatcher(None, ref_deps, cand_deps)\n",
    "    return matcher.ratio()\n",
    "\n",
    "# Lexical diversity (ONLY MTLD - we also tested TTR and MATTR)\n",
    "def lexical_diversity(text):\n",
    "    mtld = ld.mtld(text)\n",
    "    return mtld\n",
    "\n",
    "# Sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Embedding-based similarity Another alternative to BERTScore of doing this, is using the embedding similarity from the sentence transformer library.\n",
    "# def embedding_similarity(reference, candidate):\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     ref_embedding = model.encode([reference], convert_to_tensor=True)\n",
    "#     cand_embedding = model.encode([candidate], convert_to_tensor=True)\n",
    "#     similarity = cosine_similarity(ref_embedding, cand_embedding)\n",
    "#     return similarity[0][0]\n",
    "\n",
    "# N-gram overlap -its useless to use if we know that its not going to provide a valuable insights\n",
    "# def ngram_overlap(reference, candidate, n=2):\n",
    "#     ref_tokens = reference.split()\n",
    "#     cand_tokens = candidate.split()\n",
    "#     ref_ngrams = list(ngrams(ref_tokens, n))\n",
    "#     cand_ngrams = list(ngrams(cand_tokens, n))\n",
    "#     ref_counter = Counter(ref_ngrams)\n",
    "#     cand_counter = Counter(cand_ngrams)\n",
    "#     overlap = sum((ref_counter & cand_counter).values())\n",
    "#     total_ngrams = len(ref_ngrams) + len(cand_ngrams)\n",
    "#     return (2 * overlap) / total_ngrams * 100 if total_ngrams > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore specific FutureWarning about clean_up_tokenization_spaces because it is not fixable.\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message='`clean_up_tokenization_spaces`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to compute all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_v1(reference, candidate):\n",
    "\n",
    "    bert_precision, bert_recall, bert_f1 = bert_score(reference, candidate)\n",
    "    \n",
    "    syntactic_score = syntactic_similarity(reference, candidate)\n",
    "    \n",
    "    ref_mtld = lexical_diversity(reference)\n",
    "    cand_mtld = lexical_diversity(candidate)\n",
    "    \n",
    "    ref_sentiment = sentiment_analysis(reference)\n",
    "    cand_sentiment = sentiment_analysis(candidate)\n",
    "  \n",
    "    # embedding_score = embedding_similarity(reference, candidate)\n",
    "    \n",
    "    # ngram_score = ngram_overlap(reference, candidate, n=2)\n",
    "\n",
    "    # Compile all metrics into a dictionary\n",
    "    metrics = {\n",
    "        'BERTScore_Precision': bert_precision,\n",
    "        'BERTScore_Recall': bert_recall,\n",
    "        'BERTScore_F1': bert_f1,\n",
    "        'Syntactic_Similarity': syntactic_score,\n",
    "        'Reference_MTLD': ref_mtld,\n",
    "        'Candidate_MTLD': cand_mtld,\n",
    "        'Reference_Sentiment_Neg': ref_sentiment['neg'],\n",
    "        'Reference_Sentiment_Neu': ref_sentiment['neu'],\n",
    "        'Reference_Sentiment_Pos': ref_sentiment['pos'],\n",
    "        'Reference_Sentiment_Compound': ref_sentiment['compound'],\n",
    "        'Candidate_Sentiment_Neg': cand_sentiment['neg'],\n",
    "        'Candidate_Sentiment_Neu': cand_sentiment['neu'],\n",
    "        'Candidate_Sentiment_Pos': cand_sentiment['pos'],\n",
    "        'Candidate_Sentiment_Compound': cand_sentiment['compound'],\n",
    "        # 'Embedding_Similarity': embedding_score\n",
    "        # '2gram_Overlap': ngram_score,\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the function with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"test\"\n",
    "candidate = \"Not a test!\"\n",
    "calculate_metrics_v1(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reference = \"My mom drives me to school everyday. I love her very much. I am so happy.\"\n",
    "candidate = \"I go to school everyday with my mother. I truly love my mom. I am so happy!\"\n",
    "calculate_metrics_v1(reference, candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"NLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP).\"\n",
    "candidate = \"In natural language processing, n-grams are a contiguous sequence of n items from a given sample of text or speech.\"\n",
    "calculate_metrics_v1(reference, candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generating Responses\n",
    "\n",
    "Fine-tuning LLMs can be done in a few different ways. One of them is through hugging face API or as (VM et al., 2024) does. We will use the openpipe to fine-tune and call the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, start=0, limit=None):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i < start:\n",
    "                continue  # Skip lines before the starting index\n",
    "            if limit and i >= start + limit:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openpipe_response(system_msg, chat_thread, model_name, temperature_value, max_retries=3):\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    openpipe_api_key = os.getenv('OPENPIPE_API_KEY')\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        openpipe={\n",
    "            \"api_key\": openpipe_api_key,\n",
    "            \"base_url\": \"https://app.openpipe.ai/api/v1\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Combine system message and chat history\n",
    "    chat_thread = [{\"role\": \"system\", \"content\": system_msg}] + chat_thread\n",
    "    \n",
    "    response_text = ''\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Call the OpenPipe model and generate response\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=chat_thread,\n",
    "                temperature=temperature_value,\n",
    "                openpipe={\n",
    "                    \"tags\": {},\n",
    "                    \"log_request\": True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Extract the response content\n",
    "            for attr_name, attr_value in response:\n",
    "                if attr_name == 'choices':\n",
    "                    for choice in attr_value:\n",
    "                        if choice.message.content is not None:\n",
    "                            response_text += choice.message.content\n",
    "            return response_text\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"All attempts failed for response generation: {e}\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_generate_response(row, model_name, temperature_value):\n",
    "    messages = row['messages']\n",
    "    \n",
    "    # Get the last assistant message (reference)\n",
    "    reference = None\n",
    "    last_assistant_index = None\n",
    "    for idx in reversed(range(len(messages))):\n",
    "        if messages[idx]['role'] == 'assistant':\n",
    "            reference = messages[idx]['content']\n",
    "            last_assistant_index = idx\n",
    "            break\n",
    "    \n",
    "    # Ensure we found a reference\n",
    "    if reference is None:\n",
    "        return None, None, None  # Return None for context, reference, and candidate\n",
    "    \n",
    "    # Exclude the last assistant message from the chat history\n",
    "    system_msg = messages[0]['content']  # First message is the system message\n",
    "    chat_thread = messages[1:last_assistant_index]  # Exclude system message and last assistant message\n",
    "    \n",
    "    # Extract context (excluding the system message and last assistant message)\n",
    "    context = ' '.join([f\"{msg['role']}: {msg['content']}\" for msg in chat_thread])\n",
    "\n",
    "    # Call the model to generate a response (candidate)\n",
    "    candidate = get_openpipe_response(system_msg, chat_thread, model_name, temperature_value)\n",
    "    \n",
    "    return context, reference, candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_results(data, model_name, temperature_value, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for row in data:\n",
    "        context, reference, candidate = extract_and_generate_response(row, model_name, temperature_value)\n",
    "        if reference and candidate:\n",
    "            metrics = calculate_metrics_v1(reference, candidate)\n",
    "            \n",
    "            # Add context, reference, and candidate to the metrics\n",
    "            metrics['Context'] = context if context else ''\n",
    "            metrics['Reference'] = reference\n",
    "            metrics['Candidate'] = candidate\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    stats = df.describe().loc[['min', 'max', 'mean']]\n",
    "\n",
    "    # print(\"Metrics Statistics (min, max, mean):\")\n",
    "    # print(stats.T)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter cell - make any changes in the following cell:\n",
    "### Takes ages to run - so I've added tracking at each step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Configuration\n",
    "temperature_value = 0.0\n",
    "\n",
    "# Define creators and their corresponding test datasets\n",
    "creators = {\n",
    "    'creator1': {\n",
    "        'test_file': '', # Add test file path for creator1\n",
    "        'models': ['Strategy A Creator 1', 'Strategy B Creator 1', 'Strategy C Creator 1']\n",
    "    },\n",
    "    'creator2': {\n",
    "        'test_file': '', # Add test file path for creator2\n",
    "        'models': ['Strategy A Creator 2', 'Strategy B Creator 2', 'Strategy C Creator 2']\n",
    "    },\n",
    "    'creator3': {\n",
    "        'test_file': '', # Add test file path for creator3\n",
    "        'models': ['Strategy A Creator 3', 'Strategy B Creator 3', 'Strategy C Creator 3']\n",
    "    }\n",
    "}\n",
    "\n",
    "data_limit = None\n",
    "data_start = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for creator, config in creators.items():\n",
    "    print(f\"\\nProcessing {creator}'s data\")\n",
    "    output_dir = f'results_{creator}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create checkpoint file path\n",
    "    checkpoint_file = os.path.join(output_dir, 'checkpoint.json')\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    completed_models = []\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            completed_models = json.load(f)\n",
    "        print(f\"Found checkpoint with completed models: {completed_models}\")\n",
    "    \n",
    "    data = load_data(config['test_file'], start=data_start, limit=data_limit)\n",
    "    \n",
    "    for i, model_name in enumerate(config['models'], 1):\n",
    "        if model_name in completed_models:\n",
    "            print(f\"Skipping {model_name} (already completed)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing model {i}/{len(config['models'])}: {model_name}\")\n",
    "        try:\n",
    "            process_and_save_results(data, model_name, temperature_value, f\"{output_dir}/Strategy_{i}.csv\")\n",
    "            \n",
    "            # Update checkpoint\n",
    "            completed_models.append(model_name)\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(completed_models, f)\n",
    "                \n",
    "            print(f\"Completed {model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nAll processing completed in {timedelta(seconds=int(time.time() - start_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter cell - make any changes in the following cell:\n",
    "metrics_to_analyze = {\n",
    "    'semantic': 'BERTScore_F1',\n",
    "    'syntax': 'Syntactic_Similarity',\n",
    "    'lexical': ['Reference_MTLD', 'Candidate_MTLD'],\n",
    "    'sentiment': ['Reference_Sentiment_Compound', 'Candidate_Sentiment_Compound']\n",
    "}\n",
    "\n",
    "# Sample size threshold for different tests\n",
    "SAMPLE_SIZE_THRESHOLD = 30  # For normality assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calling the non fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are two important things above else to note.\n",
    "##### Why are we doing it in this part of the code, and why do we limit the data?\n",
    "\n",
    "##### 1. Personally, it was easier to do it after Generating the responses of the Fine-tuned models, because building them gave me the default code (just using it again with minor modifications) required to generate the responses. \n",
    "\n",
    "##### 2. Why do we limit the data? How are we building it? To answer the RQ1 (pretrained vs fine-tuned) we also need a pretrained model. How do we show the pretrained model what to learn from or more correctly said what to replicate. I'm giving it same data from training that goes into any Strategy. Why will there be a limit? Well we are simply feeding into its system message some sample data. And the context window of LLMs are limited to about 100k tokens. Some have more or less. We also cant share sensitive data with the model so we make it in the form of any Strategy employed (Json format) just like the fine-tune models get it. The difference is that we are instructing it by appending to the initial sentence of the system message about what the format is and what it should pay attention to (to make it as fair as possible). So, with a simple token counter, we concluded that 3k rows of the training file (csv) are roughly 80k tokens, giving us the space to also prepend the actual system message that instructs the model.\n",
    "\n",
    "##### For this part we will download the model using the opensource ollama.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the first 3k rows of the training csv file (Not randomly choosing 3k rows because it will ruin the integrity of conversations). It would be redunant to make one for each Creator so we are choosing the data from the first Creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "file_path =  #filepath to the training file of the first creator\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select first 3000 rows\n",
    "sampled_df = df[:3000]\n",
    "\n",
    "# Create new filename by adding '_3k' before the extension\n",
    "new_file_path = 'content_for_sysmsg.csv'\n",
    "\n",
    "# Save to new file\n",
    "sampled_df.to_csv(new_file_path, index=False)\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Sampled dataset size: {len(sampled_df)}\")\n",
    "print(f\"Saved to: {new_file_path}\")\n",
    "# print(sampled_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the selected training data into Strategy A (one pair) format. Its the best way to keep the token count lower to maximize the amount of data included in the system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_a_pairs(df, creator):\n",
    "    jsonl_pairs = []\n",
    "    \n",
    "    # Process each row\n",
    "    for i in range(len(df) - 1):  # Go up to second-to-last row\n",
    "        current_row = df.iloc[i]\n",
    "        next_row = df.iloc[i + 1]\n",
    "        \n",
    "        # Skip if current message is from assistant (creator)\n",
    "        if current_row['sender_handle'] == creator:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have a valid user-assistant pair\n",
    "        if (current_row['sender_handle'] != creator and \n",
    "            next_row['sender_handle'] == creator):\n",
    "            \n",
    "            pair = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": current_row['text']\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": next_row['text']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            jsonl_pairs.append(pair)\n",
    "    \n",
    "    return jsonl_pairs\n",
    "\n",
    "def save_to_jsonl(jsonl_pairs, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for pair in jsonl_pairs:\n",
    "            file.write(json.dumps(pair) + '\\n')\n",
    "\n",
    "def process_and_save_pairs(df, creator, output_path):\n",
    "    # Generate the pairs and save to JSONL\n",
    "    pairs = generate_model_a_pairs(df, creator)\n",
    "    save_to_jsonl(pairs, output_path)\n",
    "    print(f\"Generated {len(pairs)} user-assistant pairs for {creator}\")\n",
    "\n",
    "# Process the sampled data\n",
    "process_and_save_pairs(sampled_df, 'creator1', 'content_for_sysmsg.jsonl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar function to get the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_llama_response(system_msg, messages, model=\"\", temperature=0): # the model is Meta Llama 3.1 8B Instruct just like the fine-tuned models. \n",
    "    # https://ollama.com/library/llama3.1:8b/blobs/8eeb52dfb3bb\n",
    "    \n",
    "    # Construct the full message list starting with system message\n",
    "    full_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_msg\n",
    "        }\n",
    "    ]\n",
    "    # Add the rest of the messages\n",
    "    full_messages.extend(messages)\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=full_messages,\n",
    "            options={\"temperature\": temperature}  # Temperature is passed in options\n",
    "        )\n",
    "        return response[\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Llama response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we are going to generate the responses against the same TESTSet as the fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filepath', 'r') as f: #change for desired path\n",
    "    lines = f.read()\n",
    "system_msg = \"System message goes here \" + lines\n",
    "\n",
    "def load_data(filepath, start=0, limit=None):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i < start:\n",
    "                continue  # Skip lines before the starting index\n",
    "            if limit and i >= start + limit:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_generate_response(row, model_name, temperature_value=0):\n",
    "    messages = row['messages']\n",
    "    # Get the last assistant message (reference)\n",
    "    reference = None\n",
    "    last_assistant_index = None\n",
    "    for idx in reversed(range(len(messages))):\n",
    "        if messages[idx]['role'] == 'assistant':\n",
    "            reference = messages[idx]['content']\n",
    "            last_assistant_index = idx\n",
    "            break\n",
    "    # Ensure we found a reference\n",
    "    if reference is None:\n",
    "        return None, None, None  # Return None for context, reference, and candidate\n",
    "    chat_thread = messages[0:last_assistant_index]  # Exclude system message and last assistant message\n",
    "    # Extract context (excluding the system message and last assistant message)\n",
    "    context = ' '.join([f\"{msg['role']}: {msg['content']}\" for msg in chat_thread])\n",
    "    # Call the model to generate a response (candidate)\n",
    "    candidate = get_llama_response(system_msg, chat_thread, model_name, temperature_value)\n",
    "    # print(candidate)\n",
    "    return context, reference, candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_results(data, model_name, temperature_value, output_csv):\n",
    "    results = []\n",
    "    # Add tqdm progress bar\n",
    "    for row in tqdm(data, desc='Processing conversations'):\n",
    "        context, reference, candidate = extract_and_generate_response(row, model_name)\n",
    "        results.append({\n",
    "            'context': context,\n",
    "            'reference': reference,\n",
    "            'candidate': candidate\n",
    "        })\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    # Save the results to a CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "data = load_data('', 0, None) # same path as before\n",
    "process_and_save_results(data, '', 0, '') # Dont forget to change to model and filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a file now with the responses of the pretraiend models. The next steps are to compute the metrics, and comparing against model one, answering to RQ1. If our assumption stands, that the fine-tuned strategies will perform better, we continue by comparing then the fine-tuned models against themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Answering to RQ1. Compare the pretrained model vs fine-tuned models of the creator 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the results and calculating the metrics externally (for the fine-tunes we did it internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data with metrics\n",
    "existing_df = pd.read_csv('') # path with results\n",
    "print(f\"Existing data shape: {existing_df.shape}\")\n",
    "\n",
    "# Load new data to append\n",
    "new_df = pd.read_csv('')  # Replace with your new data path\n",
    "print(f\"New data shape: {new_df.shape}\")\n",
    "\n",
    "# Calculate metrics only for new rows\n",
    "print(\"Calculating metrics for new data...\")\n",
    "new_metrics_results = []\n",
    "for _, row in new_df.iterrows():\n",
    "    metrics = calculate_metrics_v1(row['reference'], row['candidate'])\n",
    "    new_metrics_results.append(metrics)\n",
    "\n",
    "# Convert new metrics results to DataFrame\n",
    "new_metrics_df = pd.DataFrame(new_metrics_results)\n",
    "new_df_with_metrics = pd.concat([new_df, new_metrics_df], axis=1)\n",
    "\n",
    "# Append new data with metrics to existing data\n",
    "combined_df = pd.concat([existing_df, new_df_with_metrics], axis=0, ignore_index=True)\n",
    "print(f\"Combined data shape: {combined_df.shape}\")\n",
    "\n",
    "# Save the combined results\n",
    "combined_df.to_csv('', index=False) # path\n",
    "print(\"Combined data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_df = pd.read_csv('') # path\n",
    "\n",
    "# Calculate metrics for pretrained model\n",
    "print(\"Calculating metrics for pretrained model...\")\n",
    "metrics_results = []\n",
    "for _, row in pretrained_df.iterrows():\n",
    "    metrics = calculate_metrics_v1(row['reference'], row['candidate'])\n",
    "    metrics_results.append(metrics)  \n",
    "\n",
    "# Convert metrics results to DataFrame and keep original columns\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "pretrained_df_with_metrics = pd.concat([pretrained_df, metrics_df], axis=1)\n",
    "\n",
    "# Save the results with metrics\n",
    "pretrained_df_with_metrics.to_csv('', index=False) # path\n",
    "\n",
    "print(\"Metrics calculated and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RQ1; compare pretrained vs fine-tune across metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(base_dir='.'):\n",
    "    all_results = {}\n",
    "    \n",
    "    # Load pretrained model results\n",
    "    print(\"Loading pretrained model results...\")\n",
    "    pretrained_df = pd.read_csv('') # path\n",
    "    all_results['Pretrained'] = pretrained_df\n",
    "    \n",
    "    # Load creator1 results only\n",
    "    creator_dir = 'results_creator1'\n",
    "    creator_path = os.path.join(base_dir, creator_dir)\n",
    "    \n",
    "    # Initialize results for creator1\n",
    "    strategy_dfs = {}\n",
    "    \n",
    "    # Load all strategy files for creator1\n",
    "    for file in os.listdir(creator_path):\n",
    "        if file.startswith('Strategy_') and file.endswith('.csv'):\n",
    "            strategy_num = file.split('_')[1].split('.')[0]\n",
    "            file_path = os.path.join(creator_path, file)\n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                strategy_dfs[f'Strategy {strategy_num}'] = df\n",
    "    \n",
    "    all_results['creator1'] = strategy_dfs\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Load results and set style for incoming plots!\n",
    "results = load_results()\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Let's verify the data is loaded correctly\n",
    "print(\"Available models:\", list(results.keys()))\n",
    "for model_type, model_results in results.items():\n",
    "    if model_type == 'Pretrained':\n",
    "        print(f\"\\nPretrained model shape:\", model_results.shape)\n",
    "    else:\n",
    "        print(f\"\\nCreator: {model_type}\")\n",
    "        print(\"Available strategies:\", list(model_results.keys()))\n",
    "        for strategy, df in model_results.items():\n",
    "            print(f\"Strategy {strategy} shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'BERTScore_F1'\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle(f'{metric} Distribution by Model', fontsize=16, y=1.05)\n",
    "\n",
    "# Prepare data for plotting\n",
    "data = []\n",
    "\n",
    "# Add pretrained model data\n",
    "pretrained_values = results['Pretrained'][metric].dropna()\n",
    "for value in pretrained_values:\n",
    "    data.append({'Model': 'Pre-trained', metric: value})\n",
    "\n",
    "# Add strategy data\n",
    "for strategy_name, df in results['creator1'].items():\n",
    "    values = df[metric].dropna()\n",
    "    for value in values:\n",
    "        data.append({'Model': strategy_name, metric: value})\n",
    "\n",
    "metric_df = pd.DataFrame(data)\n",
    "\n",
    "# Create box plot\n",
    "sns.boxplot(x='Model', y=metric, data=metric_df, ax=ax)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel(metric)\n",
    "\n",
    "# Calculate and display means\n",
    "means = metric_df.groupby('Model')[metric].mean()\n",
    "for i, mean_val in enumerate(means):\n",
    "    ax.text(i, mean_val, f'{mean_val:.3f}', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "# Save statistics\n",
    "stats_output = f\"statistics_metrics.txt\"\n",
    "with open(stats_output, 'w') as f:\n",
    "    f.write(f\"{metric} Statistics\\n{'='*50}\\n\\n\")\n",
    "    summary = metric_df.groupby('Model')[metric].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Syntactic_Similarity'\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle(f'{metric} Distribution by Model', fontsize=16, y=1.05)\n",
    "\n",
    "# Prepare data for plotting\n",
    "data = []\n",
    "\n",
    "# Add pretrained model data\n",
    "pretrained_values = results['Pretrained'][metric].dropna()\n",
    "for value in pretrained_values:\n",
    "    data.append({'Model': 'Pre-trained', metric: value})\n",
    "\n",
    "# Add strategy data\n",
    "for strategy_name, df in results['creaor1'].items():\n",
    "    values = df[metric].dropna()\n",
    "    for value in values:\n",
    "        data.append({'Model': strategy_name, metric: value})\n",
    "\n",
    "metric_df = pd.DataFrame(data)\n",
    "\n",
    "# Create box plot\n",
    "sns.boxplot(x='Model', y=metric, data=metric_df, ax=ax)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel(metric)\n",
    "\n",
    "# Calculate and display means\n",
    "means = metric_df.groupby('Model')[metric].mean()\n",
    "for i, mean_val in enumerate(means):\n",
    "    ax.text(i, mean_val, f'{mean_val:.3f}', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "# Append statistics to file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\n{metric} Statistics\\n{'='*50}\\n\\n\")\n",
    "    summary = metric_df.groupby('Model')[metric].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference MTLD from a Strategy file\n",
    "reference_mtld = results['creator1']['Strategy 1']['Reference_MTLD'].dropna()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle('MTLD Distribution by Model', fontsize=16, y=1.05)\n",
    "\n",
    "# Prepare data for plotting\n",
    "data = []\n",
    "\n",
    "# Add reference data first\n",
    "for value in reference_mtld:\n",
    "    data.append({\n",
    "        'Model': 'Reference',\n",
    "        'MTLD': value\n",
    "    })\n",
    "\n",
    "# Add pretrained model data\n",
    "pretrained_values = results['Pretrained']['Candidate_MTLD'].dropna()\n",
    "for value in pretrained_values:\n",
    "    data.append({\n",
    "        'Model': 'Pre-trained',\n",
    "        'MTLD': value\n",
    "    })\n",
    "\n",
    "# Add strategy data\n",
    "for strategy_name, df in results['creator1'].items():\n",
    "    values = df['Candidate_MTLD'].dropna()\n",
    "    for value in values:\n",
    "        data.append({\n",
    "            'Model': strategy_name,\n",
    "            'MTLD': value\n",
    "        })\n",
    "\n",
    "metric_df = pd.DataFrame(data)\n",
    "\n",
    "# Create box plot\n",
    "sns.boxplot(x='Model', y='MTLD', data=metric_df, ax=ax)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('MTLD Score')\n",
    "\n",
    "# Calculate and display means\n",
    "means = metric_df.groupby('Model')['MTLD'].mean()\n",
    "for i, mean_val in enumerate(means):\n",
    "    ax.text(i, mean_val, f'{mean_val:.3f}', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "# Append statistics to file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\nMTLD Statistics\\n{'='*50}\\n\\n\")\n",
    "    summary = metric_df.groupby('Model')['MTLD'].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference Sentiment from a Strategy \n",
    "reference_sentiment = results['creator1']['Strategy 1']['Reference_Sentiment_Compound'].dropna()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle('Sentiment Compound Score Distribution by Model', fontsize=16, y=1.05)\n",
    "\n",
    "# Prepare data for plotting\n",
    "data = []\n",
    "\n",
    "# Add reference data first\n",
    "for value in reference_sentiment:\n",
    "    data.append({\n",
    "        'Strategy': 'Reference',\n",
    "        'Sentiment': value\n",
    "    })\n",
    "\n",
    "# Add pretrained model data\n",
    "pretrained_values = results['Pretrained']['Candidate_Sentiment_Compound'].dropna()\n",
    "for value in pretrained_values:\n",
    "    data.append({\n",
    "        'Strategy': 'Pre-trained',\n",
    "        'Sentiment': value\n",
    "    })\n",
    "\n",
    "# Add strategy data\n",
    "for strategy_name, df in results['creator1'].items():\n",
    "    values = df['Candidate_Sentiment_Compound'].dropna()\n",
    "    for value in values:\n",
    "        data.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'Sentiment': value\n",
    "        })\n",
    "\n",
    "metric_df = pd.DataFrame(data)\n",
    "\n",
    "# Create box plot\n",
    "sns.boxplot(x='Strategy', y='Sentiment', data=metric_df, ax=ax)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Sentiment Compound Score')\n",
    "\n",
    "# Calculate and display means\n",
    "means = metric_df.groupby('Strategy')['Sentiment'].mean()\n",
    "for i, mean_val in enumerate(means):\n",
    "    ax.text(i, mean_val, f'{mean_val:.3f}', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "# Append statistics to file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\nSentiment Statistics\\n{'='*50}\\n\\n\")\n",
    "    summary = metric_df.groupby('Strategy')['Sentiment'].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 3 subplots (one for each sentiment component)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Sentiment Distribution by Component', fontsize=16, y=1.05)\n",
    "\n",
    "# Define sentiment components\n",
    "sentiment_components = ['Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos']\n",
    "titles = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# Get reference sentiment from Strategy 1\n",
    "reference_data = results['creator1']['Strategy 1']\n",
    "\n",
    "for idx, (component, title) in enumerate(zip(sentiment_components, titles)):\n",
    "    data = []\n",
    "    \n",
    "    # Add reference data\n",
    "    ref_values = reference_data[f'Reference_{component}'].dropna()\n",
    "    for value in ref_values:\n",
    "        data.append({\n",
    "            'Strategy': 'Reference',\n",
    "            'Score': value\n",
    "        })\n",
    "    \n",
    "    # Add pretrained model data\n",
    "    pretrained_values = results['Pretrained'][f'Candidate_{component}'].dropna()\n",
    "    for value in pretrained_values:\n",
    "        data.append({\n",
    "            'Strategy': 'Pre-trained',\n",
    "            'Score': value\n",
    "        })\n",
    "    \n",
    "    # Add strategy data\n",
    "    for strategy_name, df in results['creator1'].items():\n",
    "        values = df[f'Candidate_{component}'].dropna()\n",
    "        for value in values:\n",
    "            data.append({\n",
    "                'Strategy': strategy_name,\n",
    "                'Score': value\n",
    "            })\n",
    "    \n",
    "    metric_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(x='Strategy', y='Score', data=metric_df, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{title} Sentiment')\n",
    "    axes[idx].set_xlabel('Model')\n",
    "    axes[idx].set_ylabel('Score' if idx == 0 else '')\n",
    "    \n",
    "    # Calculate and display means slightly to the right of the boxes\n",
    "    means = metric_df.groupby('Strategy')['Score'].mean()\n",
    "    for i, mean_val in enumerate(means):\n",
    "        axes[idx].text(i + 0.1, mean_val, f'{mean_val:.3f}', \n",
    "                      horizontalalignment='left', \n",
    "                      verticalalignment='bottom')\n",
    "    \n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1: Compare pretrained model with Creator 1's strategies\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle('RQ1: Sentiment Vector Distance Analysis (Creator 1)', fontsize=16, y=1.05)\n",
    "\n",
    "distances = []\n",
    "\n",
    "# Get reference from Creator 1's first strategy\n",
    "first_strategy = list(results['creator1'].keys())[0]\n",
    "ref_df = results['creator1'][first_strategy]\n",
    "\n",
    "# Load pretrained model results separately\n",
    "pretrained_df = pd.read_csv('#path')\n",
    "\n",
    "# Calculate distances for pretrained model\n",
    "for i in range(len(pretrained_df)):\n",
    "    ref_point = np.array([ref_df['Reference_Sentiment_Pos'].iloc[i],\n",
    "                         ref_df['Reference_Sentiment_Neu'].iloc[i],\n",
    "                         ref_df['Reference_Sentiment_Neg'].iloc[i]])\n",
    "    \n",
    "    cand_point = np.array([pretrained_df['Candidate_Sentiment_Pos'].iloc[i],\n",
    "                          pretrained_df['Candidate_Sentiment_Neu'].iloc[i],\n",
    "                          pretrained_df['Candidate_Sentiment_Neg'].iloc[i]])\n",
    "    \n",
    "    distance = np.linalg.norm(ref_point - cand_point)\n",
    "    distances.append({\n",
    "        'Model': 'Pre-trained',\n",
    "        'Distance': distance\n",
    "    })\n",
    "\n",
    "# Calculate distances for Creator 1's strategies\n",
    "for strategy_name, df in results['creator1'].items():\n",
    "    for i in range(len(df)):\n",
    "        ref_point = np.array([df['Reference_Sentiment_Pos'].iloc[i],\n",
    "                            df['Reference_Sentiment_Neu'].iloc[i],\n",
    "                            df['Reference_Sentiment_Neg'].iloc[i]])\n",
    "        \n",
    "        cand_point = np.array([df['Candidate_Sentiment_Pos'].iloc[i],\n",
    "                             df['Candidate_Sentiment_Neu'].iloc[i],\n",
    "                             df['Candidate_Sentiment_Neg'].iloc[i]])\n",
    "        \n",
    "        distance = np.linalg.norm(ref_point - cand_point)\n",
    "        distances.append({\n",
    "            'Model': strategy_name,\n",
    "            'Distance': distance\n",
    "        })\n",
    "\n",
    "# Create violin plot\n",
    "distance_df = pd.DataFrame(distances)\n",
    "sns.violinplot(x='Model', y='Distance', data=distance_df, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Sentiment Vector Distance')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Starting to answer RQ2. Plotting for all creators at once. Comparing the fine-tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the results. Making Sure everything is on point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(base_dir='.'):\n",
    "    all_results = {}\n",
    "    \n",
    "    # Look for results_{creator} directories\n",
    "    for creator_dir in ['results_creator1', 'results_creator2', 'results_creator3']:\n",
    "        creator_name = creator_dir.replace('results_', '')\n",
    "        creator_path = os.path.join(base_dir, creator_dir) # path\n",
    "        \n",
    "        # Initialize results for this creator\n",
    "        strategy_dfs = {}\n",
    "        \n",
    "        # Load all strategy files for this creator\n",
    "        for file in os.listdir(creator_path):\n",
    "            if file.startswith('Strategy_') and file.endswith('.csv'):\n",
    "                strategy_num = file.split('_')[1].split('.')[0]\n",
    "                file_path = os.path.join(creator_path, file) # path\n",
    "                \n",
    "                if os.path.isfile(file_path):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    strategy_dfs[f'Strategy {strategy_num}'] = df\n",
    "        \n",
    "        all_results[creator_name] = strategy_dfs\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Load results and set style\n",
    "results = load_results()\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Let's verify the data is loaded correctly\n",
    "print(\"Available creators:\", list(results.keys()))\n",
    "for creator, creator_results in results.items():\n",
    "    print(f\"\\nCreator: {creator}\")\n",
    "    print(\"Available strategies:\", list(creator_results.keys()))\n",
    "    for strategy, df in creator_results.items():\n",
    "        print(f\"Strategy {strategy} shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'BERTScore_F1'\n",
    "\n",
    "# Create figure with 3 subplots (one for each creator)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle(f'{metric} Distribution by Strategy', fontsize=16, y=1.05)\n",
    "\n",
    "# Prepare statistics file\n",
    "stats_output = f\"statistics_{metric}.txt\"\n",
    "with open(stats_output, 'w') as f:\n",
    "    f.write(f\"{metric} Statistics\\n{'='*50}\\n\\n\")\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for strategy_name, df in creator_results.items():\n",
    "        values = df[metric].dropna()\n",
    "        for value in values:\n",
    "            data.append({'Strategy': strategy_name, metric: value})\n",
    "    \n",
    "    metric_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(x='Strategy', y=metric, data=metric_df, ax=axes[idx-1])\n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel(metric if idx == 1 else '')\n",
    "    \n",
    "    # Calculate and display means\n",
    "    means = metric_df.groupby('Strategy')[metric].mean()\n",
    "    for i, mean_val in enumerate(means):\n",
    "        axes[idx-1].text(i, mean_val, f'{mean_val:.3f}', \n",
    "                      horizontalalignment='center', \n",
    "                      verticalalignment='bottom')\n",
    "    \n",
    "    # Save statistics to file\n",
    "    with open(stats_output, 'a') as f:\n",
    "        f.write(f\"Creator {idx}\\n{'-'*20}\\n\")\n",
    "        summary = metric_df.groupby('Strategy')[metric].agg(['count', 'mean', 'std', 'min', 'max']).round(4)\n",
    "        f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Syntactic_Similarity'\n",
    "\n",
    "# Create figure with 3 subplots (one for each creator)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle(f'{metric} Distribution by Strategy', fontsize=16, y=1.05)\n",
    "\n",
    "# Append to statistics file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\n{metric} Statistics\\n{'='*50}\\n\\n\")\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for strategy_name, df in creator_results.items():\n",
    "        values = df[metric].dropna()\n",
    "        for value in values:\n",
    "            data.append({'Strategy': strategy_name, metric: value})\n",
    "    \n",
    "    metric_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(x='Strategy', y=metric, data=metric_df, ax=axes[idx-1])\n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel(metric if idx == 1 else '')\n",
    "    \n",
    "    # Calculate and display means\n",
    "    means = metric_df.groupby('Strategy')[metric].mean()\n",
    "    for i, mean_val in enumerate(means):\n",
    "        axes[idx-1].text(i, mean_val, f'{mean_val:.3f}', \n",
    "                      horizontalalignment='center', \n",
    "                      verticalalignment='bottom')\n",
    "    \n",
    "    # Save statistics to file\n",
    "    with open(stats_output, 'a') as f:\n",
    "        f.write(f\"Creator {idx}\\n{'-'*20}\\n\")\n",
    "        summary = metric_df.groupby('Strategy')[metric].agg(['count', 'mean', 'std', 'min', 'max']).round(4)\n",
    "        f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Reference_MTLD', 'Candidate_MTLD']\n",
    "\n",
    "# Create figure with 3 subplots (one for each creator)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('MTLD Distribution by Strategy', fontsize=16, y=1.05)\n",
    "\n",
    "# Append to statistics file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\nMTLD Statistics\\n{'='*50}\\n\\n\")\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for strategy_name, df in creator_results.items():\n",
    "        for mtld_metric in metrics:\n",
    "            values = df[mtld_metric].dropna()\n",
    "            for value in values:\n",
    "                data.append({\n",
    "                    'Strategy': strategy_name,\n",
    "                    'MTLD_Type': mtld_metric.replace('_MTLD', ''),\n",
    "                    'MTLD': value\n",
    "                })\n",
    "    \n",
    "    metric_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(x='Strategy', y='MTLD', hue='MTLD_Type', data=metric_df, ax=axes[idx-1])\n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel('MTLD' if idx == 1 else '')\n",
    "    \n",
    "    # Save statistics to file\n",
    "    with open(stats_output, 'a') as f:\n",
    "        f.write(f\"Creator {idx}\\n{'-'*20}\\n\")\n",
    "        summary = metric_df.groupby(['Strategy', 'MTLD_Type'])['MTLD'].agg(['count', 'mean', 'std', 'min', 'max']).round(4)\n",
    "        f.write(f\"{summary.to_string()}\\n\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 3 subplots (one for each creator)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('MTLD Distribution by Strategy with Reference Comparison', fontsize=16, y=1.05)\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items()):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    \n",
    "    # Get the first strategy (whatever its name is)\n",
    "    first_strategy = list(creator_results.keys())[0]\n",
    "    \n",
    "    # Add reference data from first strategy\n",
    "    reference_values = creator_results[first_strategy]['Reference_MTLD'].dropna()\n",
    "    for value in reference_values:\n",
    "        data.append({\n",
    "            'Strategy': 'Reference',\n",
    "            'MTLD': value\n",
    "        })\n",
    "    \n",
    "    # Add strategy data\n",
    "    for strategy_name, df in creator_results.items():\n",
    "        values = df['Candidate_MTLD'].dropna()\n",
    "        for value in values:\n",
    "            data.append({\n",
    "                'Strategy': strategy_name,\n",
    "                'MTLD': value\n",
    "            })\n",
    "    \n",
    "    metric_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(x='Strategy', y='MTLD', data=metric_df, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Creator {idx+1}')\n",
    "    axes[idx].set_xlabel('Strategy')\n",
    "    axes[idx].set_ylabel('MTLD Score' if idx == 0 else '')\n",
    "    \n",
    "    # Calculate and display means with slightly higher vertical offset\n",
    "    means = metric_df.groupby('Strategy')['MTLD'].mean()\n",
    "    for i, mean_val in enumerate(means):\n",
    "        axes[idx].text(i, mean_val + 0.7, f'{mean_val:.3f}', \n",
    "                      horizontalalignment='center', \n",
    "                      verticalalignment='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save statistics to file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\nMTLD Statistics with Reference Comparison\\n{'='*50}\\n\\n\")\n",
    "    for creator, creator_results in results.items():\n",
    "        f.write(f\"\\nCreator: {creator}\\n{'-'*20}\\n\")\n",
    "        \n",
    "        # Prepare data for statistics\n",
    "        data = []\n",
    "        # Add reference data from first strategy\n",
    "        first_strategy = list(creator_results.keys())[0]\n",
    "        reference_values = creator_results[first_strategy]['Reference_MTLD'].dropna()\n",
    "        data.append({\n",
    "            'Strategy': 'Reference',\n",
    "            'Values': reference_values\n",
    "        })\n",
    "        \n",
    "        # Add strategy data\n",
    "        for strategy_name, df in creator_results.items():\n",
    "            data.append({\n",
    "                'Strategy': strategy_name,\n",
    "                'Values': df['Candidate_MTLD'].dropna()\n",
    "            })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        for item in data:\n",
    "            values = item['Values']\n",
    "            f.write(f\"\\n{item['Strategy']}:\\n\")\n",
    "            f.write(f\"Mean: {values.mean():.4f}\\n\")\n",
    "            f.write(f\"Std: {values.std():.4f}\\n\")\n",
    "            f.write(f\"Min: {values.min():.4f}\\n\")\n",
    "            f.write(f\"Max: {values.max():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 3 subplots for average MTLD values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Average MTLD by Strategy', fontsize=16, y=1.05)\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for strategy_name, df in creator_results.items():\n",
    "        for mtld_metric in metrics:\n",
    "            values = df[mtld_metric].dropna()\n",
    "            data.append({\n",
    "                'Strategy': strategy_name,\n",
    "                'MTLD_Type': mtld_metric.replace('_MTLD', ''),\n",
    "                'MTLD': values.mean(),\n",
    "                'std': values.std(),\n",
    "                'count': len(values)\n",
    "            })\n",
    "    \n",
    "    metric_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    confidence = 0.95\n",
    "    metric_df['sem'] = metric_df['std'] / np.sqrt(metric_df['count'])\n",
    "    metric_df['h'] = metric_df['sem'] * stats.t.ppf((1 + confidence) / 2., metric_df['count'] - 1)\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(x='Strategy', y='MTLD', hue='MTLD_Type', data=metric_df, ax=axes[idx-1], capsize=0.2)\n",
    "    \n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel('Average MTLD' if idx == 1 else '')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound Sentiment Comparison across creators\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Sentiment Compound Score Distribution by Strategy', fontsize=16, y=1.05)\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    plot_data = []\n",
    "    \n",
    "    # Get reference data from Strategy 1 (consistent reference)\n",
    "    reference_values = creator_results['Strategy 1']['Reference_Sentiment_Compound'].dropna()\n",
    "    for value in reference_values:\n",
    "        plot_data.append({\n",
    "            'Strategy': 'Reference',\n",
    "            'Compound': value\n",
    "        })\n",
    "    \n",
    "    # Add strategy data\n",
    "    for strategy_name, df in creator_results.items():\n",
    "        values = df['Candidate_Sentiment_Compound'].dropna()\n",
    "        for value in values:\n",
    "            plot_data.append({\n",
    "                'Strategy': strategy_name,\n",
    "                'Compound': value\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(x='Strategy', y='Compound', data=plot_df, ax=axes[idx-1])\n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel('Compound Score' if idx == 1 else '')\n",
    "    \n",
    "    # Add means as text, shifted slightly to the right\n",
    "    means = plot_df.groupby('Strategy')['Compound'].mean()\n",
    "    for i, mean_val in enumerate(means):\n",
    "        axes[idx-1].text(i + 0.2, mean_val, f'{mean_val:.3f}', \n",
    "                        horizontalalignment='center', \n",
    "                        verticalalignment='bottom')\n",
    "    \n",
    "    axes[idx-1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Components (Pos, Neg, Neu) Comparison\n",
    "sentiment_components = ['Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos']\n",
    "titles = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))  # 3 rows (one per component) x 3 cols (one per creator)\n",
    "fig.suptitle('Sentiment Components Distribution by Strategy', fontsize=16, y=1.02)\n",
    "\n",
    "for row, (component, title) in enumerate(zip(sentiment_components, titles)):\n",
    "    for col, (creator, creator_results) in enumerate(results.items()):\n",
    "        plot_data = []\n",
    "        \n",
    "        # Get reference data from Strategy 1\n",
    "        ref_values = creator_results['Strategy 1'][f'Reference_{component}'].dropna()\n",
    "        for value in ref_values:\n",
    "            plot_data.append({\n",
    "                'Strategy': 'Reference',\n",
    "                'Score': value\n",
    "            })\n",
    "        \n",
    "        # Add strategy data\n",
    "        for strategy_name, df in creator_results.items():\n",
    "            values = df[f'Candidate_{component}'].dropna()\n",
    "            for value in values:\n",
    "                plot_data.append({\n",
    "                    'Strategy': strategy_name,\n",
    "                    'Score': value\n",
    "                })\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Create box plot\n",
    "        sns.boxplot(x='Strategy', y='Score', data=plot_df, ax=axes[row, col])\n",
    "        \n",
    "        # Set titles and labels\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f'Creator {col+1}')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(f'{title} Score')\n",
    "        else:\n",
    "            axes[row, col].set_ylabel('')\n",
    "            \n",
    "        axes[row, col].set_xlabel('')\n",
    "        \n",
    "        # Add means as text, shifted slightly to the right\n",
    "        means = plot_df.groupby('Strategy')['Score'].mean()\n",
    "        for i, mean_val in enumerate(means):\n",
    "            axes[row, col].text(i + 0.2, mean_val, f'{mean_val:.3f}', \n",
    "                              horizontalalignment='left', \n",
    "                              verticalalignment='bottom')\n",
    "        \n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_metrics = ['Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos', 'Sentiment_Compound']\n",
    "\n",
    "# Append to statistics file\n",
    "stats_output = \"statistics_metrics.txt\"\n",
    "with open(stats_output, 'a') as f:\n",
    "    f.write(f\"\\n\\nSentiment Analysis Statistics\\n{'='*50}\\n\\n\")\n",
    "\n",
    "# Create figure with 3 subplots for Compound sentiment (as a basic view)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Sentiment Compound Score Distribution by Strategy', fontsize=16, y=1.05)\n",
    "\n",
    "# Plot for each creator\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    # Prepare data and calculate statistics for all sentiment metrics\n",
    "    stats_data = []\n",
    "    plot_data = []\n",
    "    \n",
    "    for strategy_name, df in creator_results.items():\n",
    "        # For plotting (just compound scores)\n",
    "        ref_compound = df['Reference_Sentiment_Compound'].dropna()\n",
    "        cand_compound = df['Candidate_Sentiment_Compound'].dropna()\n",
    "        \n",
    "        for value in ref_compound:\n",
    "            plot_data.append({'Strategy': strategy_name, 'Type': 'Reference', 'Compound': value})\n",
    "        for value in cand_compound:\n",
    "            plot_data.append({'Strategy': strategy_name, 'Type': 'Candidate', 'Compound': value})\n",
    "        \n",
    "        # For statistics (all sentiment metrics)\n",
    "        for prefix in ['Reference', 'Candidate']:\n",
    "            stats = {\n",
    "                'Strategy': strategy_name,\n",
    "                'Type': prefix\n",
    "            }\n",
    "            for metric in sentiment_metrics:\n",
    "                col_name = f'{prefix}_{metric}'\n",
    "                if col_name in df.columns:\n",
    "                    stats[metric] = df[col_name].mean()\n",
    "            stats_data.append(stats)\n",
    "    \n",
    "    # Save statistics to file\n",
    "    with open(stats_output, 'a') as f:\n",
    "        f.write(f\"Creator {idx}\\n{'-'*20}\\n\")\n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        f.write(f\"{stats_df.to_string()}\\n\\n\")\n",
    "    \n",
    "    # Create box plot for compound scores\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    sns.boxplot(x='Strategy', y='Compound', hue='Type', data=plot_df, ax=axes[idx-1])\n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel('Compound Score' if idx == 1 else '')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Distance from Reference Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Sentiment Distance from Reference', fontsize=16, y=1.05)\n",
    "\n",
    "for idx, (creator, creator_results) in enumerate(results.items(), 1):\n",
    "    distances = []\n",
    "    \n",
    "    for strategy_name, df in creator_results.items():\n",
    "        # Calculate Euclidean distance in sentiment space\n",
    "        for i in range(len(df)):\n",
    "            ref_point = np.array([df['Reference_Sentiment_Pos'].iloc[i],\n",
    "                                df['Reference_Sentiment_Neu'].iloc[i],\n",
    "                                df['Reference_Sentiment_Neg'].iloc[i]])\n",
    "            \n",
    "            cand_point = np.array([df['Candidate_Sentiment_Pos'].iloc[i],\n",
    "                                 df['Candidate_Sentiment_Neu'].iloc[i],\n",
    "                                 df['Candidate_Sentiment_Neg'].iloc[i]])\n",
    "            \n",
    "            distance = np.linalg.norm(ref_point - cand_point)\n",
    "            distances.append({\n",
    "                'Strategy': strategy_name,\n",
    "                'Distance': distance\n",
    "            })\n",
    "    \n",
    "    # Create violin plot of distances\n",
    "    distance_df = pd.DataFrame(distances)\n",
    "    sns.violinplot(x='Strategy', y='Distance', data=distance_df, ax=axes[idx-1])\n",
    "    \n",
    "    axes[idx-1].set_title(f'Creator {idx}')\n",
    "    axes[idx-1].set_xlabel('Strategy')\n",
    "    axes[idx-1].set_ylabel('Distance from Reference' if idx == 1 else '')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normality(data, metric_name):\n",
    "    # Shapiro-Wilk test for each strategy and creator\n",
    "    for creator, creator_results in results.items():\n",
    "        print(f\"\\nNormality Tests for {metric_name} - {creator}\")\n",
    "        for strategy, df in creator_results.items():\n",
    "            stat, p_value = stats.shapiro(df[metric_name].dropna())\n",
    "            print(f\"{strategy}: p-value = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(metric_name):\n",
    "    print(f\"\\n{'='*20} Statistical Analysis for {metric_name} {'='*20}\\n\")\n",
    "    \n",
    "    # 1. Normality Tests\n",
    "    print(\"1. Normality Test (Shapiro-Wilk)\")\n",
    "    print(\"---------------------------------\")\n",
    "    all_normal = True\n",
    "    \n",
    "    for creator, creator_results in results.items():\n",
    "        print(f\"\\nCreator: {creator}\")\n",
    "        for strategy, df in creator_results.items():\n",
    "            stat, p_value = stats.shapiro(df[metric_name].dropna())\n",
    "            is_normal = p_value > 0.05\n",
    "            all_normal = all_normal and is_normal\n",
    "            print(f\"{strategy}: p-value = {p_value:.4f} ({'Normal' if is_normal else 'Non-normal'})\")\n",
    "    \n",
    "    # 2. Strategy Comparison Tests\n",
    "    print(\"\\n2. Strategy Comparison\")\n",
    "    print(\"----------------------\")\n",
    "    \n",
    "    for creator, creator_results in results.items():\n",
    "        print(f\"\\nCreator: {creator}\")\n",
    "        \n",
    "        # Prepare data for comparison\n",
    "        strategy_data = [df[metric_name].dropna() for df in creator_results.values()]\n",
    "        \n",
    "        if all_normal:\n",
    "            # Use one-way ANOVA for normal distributions\n",
    "            f_stat, p_value = stats.f_oneway(*strategy_data)\n",
    "            print(f\"One-way ANOVA: p-value = {p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                # Post-hoc Tukey test\n",
    "                \n",
    "                data = []\n",
    "                groups = []\n",
    "                for i, (strategy, df) in enumerate(creator_results.items()):\n",
    "                    data.extend(df[metric_name].dropna())\n",
    "                    groups.extend([f\"Strategy {i+1}\"] * len(df[metric_name].dropna()))\n",
    "                \n",
    "                tukey = pairwise_tukeyhsd(data, groups)\n",
    "                print(\"\\nTukey's HSD test:\")\n",
    "                print(tukey)\n",
    "        else:\n",
    "            # Use Kruskal-Wallis H-test for non-normal distributions\n",
    "            h_stat, p_value = stats.kruskal(*strategy_data)\n",
    "            print(f\"Kruskal-Wallis H-test: p-value = {p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                # Post-hoc Mann-Whitney U tests with Bonferroni correction\n",
    "                print(\"\\nPairwise Mann-Whitney U tests (with Bonferroni correction):\")\n",
    "                strategies = list(creator_results.keys())\n",
    "                for i in range(len(strategies)):\n",
    "                    for j in range(i+1, len(strategies)):\n",
    "                        stat, p = stats.mannwhitneyu(\n",
    "                            creator_results[strategies[i]][metric_name].dropna(),\n",
    "                            creator_results[strategies[j]][metric_name].dropna()\n",
    "                        )\n",
    "                        # Bonferroni correction\n",
    "                        p_adjusted = p * (len(strategies) * (len(strategies)-1) / 2)\n",
    "                        print(f\"{strategies[i]} vs {strategies[j]}: p-value = {p_adjusted:.4f}\")\n",
    "    \n",
    "    # 3. Effect Size Analysis\n",
    "    print(\"\\n3. Effect Size Analysis\")\n",
    "    print(\"----------------------\")\n",
    "    for creator, creator_results in results.items():\n",
    "        print(f\"\\nCreator: {creator}\")\n",
    "        strategies = list(creator_results.keys())\n",
    "        for i in range(len(strategies)):\n",
    "            for j in range(i+1, len(strategies)):\n",
    "                d = cohen_d(\n",
    "                    creator_results[strategies[i]][metric_name].dropna(),\n",
    "                    creator_results[strategies[j]][metric_name].dropna()\n",
    "                )\n",
    "                print(f\"Cohen's d ({strategies[i]} vs {strategies[j]}): {d:.4f}\")\n",
    "\n",
    "# Helper function for Cohen's d\n",
    "def cohen_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    return (x.mean() - y.mean()) / np.sqrt(((nx-1)*x.std()**2 + (ny-1)*y.std()**2) / dof)\n",
    "\n",
    "# Run analysis for each metric\n",
    "metrics_to_analyze = ['BERTScore_F1', 'Syntactic_Similarity', 'Reference_MTLD', 'Candidate_MTLD', 'Reference_Sentiment_Compound', 'Candidate_Sentiment_Compound']\n",
    "\n",
    "for metric in metrics_to_analyze:\n",
    "    perform_statistical_analysis(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Data Quality Check\n",
    "def check_data_quality():\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"==================\\n\")\n",
    "    \n",
    "    for creator, creator_results in results.items():\n",
    "        print(f\"Creator: {creator}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        for strategy, df in creator_results.items():\n",
    "            print(f\"\\nStrategy: {strategy}\")\n",
    "            print(f\"Sample size: {len(df)}\")\n",
    "            print(\"\\nMissing values:\")\n",
    "            print(df.isnull().sum())\n",
    "            print(\"\\nBasic statistics:\")\n",
    "            print(df.describe().round(4))\n",
    "        print(\"\\n\")\n",
    "\n",
    "check_data_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Cross-Metric Correlation Analysis and Visualization\n",
    "\n",
    "\n",
    "def analyze_metric_correlations():\n",
    "    print(\"CROSS-METRIC CORRELATION ANALYSIS\")\n",
    "    print(\"================================\\n\")\n",
    "    \n",
    "    correlation_metrics = [\n",
    "        'BERTScore_F1',\n",
    "        'Syntactic_Similarity',\n",
    "        'Candidate_MTLD',\n",
    "        'Candidate_Sentiment_Compound'\n",
    "    ]\n",
    "    \n",
    "    for creator, creator_results in results.items():\n",
    "        print(f\"Creator: {creator}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        for strategy, df in creator_results.items():\n",
    "            print(f\"\\nStrategy: {strategy}\")\n",
    "            \n",
    "            # Correlation matrix\n",
    "            corr_matrix = df[correlation_metrics].corr()\n",
    "            print(\"\\nCorrelation Matrix:\")\n",
    "            print(corr_matrix.round(4))\n",
    "            \n",
    "            # Create heatmap\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "            plt.title(f'Correlation Matrix - {creator} - {strategy}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Statistical significance\n",
    "            print(\"\\nCorrelation P-values:\")\n",
    "            p_values = pd.DataFrame(index=correlation_metrics, columns=correlation_metrics)\n",
    "            for i in correlation_metrics:\n",
    "                for j in correlation_metrics:\n",
    "                    if i != j:\n",
    "                        stat, p = stats.pearsonr(\n",
    "                            df[i].dropna().values,\n",
    "                            df[j].dropna().values\n",
    "                        )\n",
    "                        p_values.loc[i, j] = p\n",
    "            print(p_values.round(4))\n",
    "        print(\"\\n\")\n",
    "\n",
    "analyze_metric_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_power_analysis():\n",
    "    print(\"STATISTICAL POWER ANALYSIS\")\n",
    "    print(\"=========================\\n\")\n",
    "    \n",
    "    # Parameters\n",
    "    alpha = 0.05\n",
    "    sample_sizes = []\n",
    "    effect_sizes = []\n",
    "    \n",
    "    metrics_list = [\n",
    "        'BERTScore_F1',\n",
    "        'Syntactic_Similarity',\n",
    "        'Candidate_MTLD',\n",
    "        'Candidate_Sentiment_Compound'\n",
    "    ]\n",
    "    \n",
    "    for creator, creator_results in results.items():\n",
    "        for metric in metrics_list:\n",
    "            strategies = list(creator_results.keys())\n",
    "            for i in range(len(strategies)-1):\n",
    "                for j in range(i+1, len(strategies)):\n",
    "                    data1 = creator_results[strategies[i]][metric].dropna()\n",
    "                    data2 = creator_results[strategies[j]][metric].dropna()\n",
    "                    \n",
    "                    sample_sizes.append(min(len(data1), len(data2)))\n",
    "                    effect_size = abs(cohen_d(data1, data2))\n",
    "                    if not np.isnan(effect_size):  # Check for valid effect size\n",
    "                        effect_sizes.append(effect_size)\n",
    "    \n",
    "    if effect_sizes:  # Check if we have valid effect sizes\n",
    "        mean_effect_size = float(np.mean(effect_sizes))  # Convert to float\n",
    "        mean_sample_size = float(np.mean(sample_sizes))  # Convert to float\n",
    "        \n",
    "        power_analysis = TTestPower()\n",
    "        power = power_analysis.power(\n",
    "            effect_size=mean_effect_size,\n",
    "            nobs=mean_sample_size,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "        print(f\"Average effect size: {mean_effect_size:.4f}\")\n",
    "        print(f\"Average sample size: {int(mean_sample_size)}\")\n",
    "        print(f\"Statistical power: {power:.4f}\")\n",
    "        \n",
    "        required_n = power_analysis.solve_power(\n",
    "            effect_size=mean_effect_size,\n",
    "            power=0.8,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        print(f\"Required sample size for 80% power: {int(required_n)}\")\n",
    "    else:\n",
    "        print(\"No valid effect sizes calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_report():\n",
    "    print(\"SUMMARY STATISTICS REPORT\")\n",
    "    print(\"========================\\n\")\n",
    "    \n",
    "    # Define metrics to analyze (use the same list as before)\n",
    "    metrics_list = [\n",
    "        'BERTScore_F1',\n",
    "        'Syntactic_Similarity',\n",
    "        'Candidate_MTLD',\n",
    "        'Candidate_Sentiment_Compound'\n",
    "    ]\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for creator, creator_results in results.items():\n",
    "        for strategy, df in creator_results.items():\n",
    "            for metric in metrics_list:  # Use metrics_list instead of metrics_to_analyze\n",
    "                summary = {\n",
    "                    'Creator': creator,\n",
    "                    'Strategy': strategy,\n",
    "                    'Metric': metric,\n",
    "                    'Mean': df[metric].mean(),\n",
    "                    'Std': df[metric].std(),\n",
    "                    'Median': df[metric].median(),\n",
    "                    'N': len(df[metric].dropna())\n",
    "                }\n",
    "                summary_data.append(summary)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"Overall Summary Statistics:\")\n",
    "    print(summary_df.round(4))\n",
    "    \n",
    "    # Create summary plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    fig.suptitle('Summary of Key Metrics Across Strategies and Creators')\n",
    "    \n",
    "    # Plot means with error bars\n",
    "    for i, metric in enumerate(metrics_list):  # Use metrics_list here too\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        sns.barplot(data=summary_df[summary_df['Metric'] == metric],\n",
    "                   x='Strategy', y='Mean', hue='Creator',\n",
    "                   ax=axes[row, col])\n",
    "        \n",
    "        axes[row, col].set_title(f'{metric} by Strategy and Creator')\n",
    "        axes[row, col].set_ylabel('Mean Value')\n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 3 subplots for the different metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Summary of Key Metrics Across Strategies and Creators', fontsize=16, y=1.05)\n",
    "\n",
    "metrics = {\n",
    "    'BERTScore_F1': 'BERTScore',\n",
    "    'Syntactic_Similarity': 'Syntactic Similarity', \n",
    "    'Candidate_MTLD': 'MTLD'\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "summary_data = []\n",
    "\n",
    "for creator, creator_results in results.items():\n",
    "    creator_num = f'Creator {list(results.keys()).index(creator) + 1}'\n",
    "    \n",
    "    for strategy, df in creator_results.items():\n",
    "        for metric_key, metric_name in metrics.items():\n",
    "            # Calculate median instead of mean to account for outliers\n",
    "            summary = {\n",
    "                'Creator': creator_num,\n",
    "                'Strategy': strategy,\n",
    "                'Metric': metric_name,\n",
    "                'Value': df[metric_key].median()\n",
    "            }\n",
    "            summary_data.append(summary)\n",
    "            \n",
    "            # Add reference MTLD if this is the MTLD metric\n",
    "            if metric_key == 'Candidate_MTLD':\n",
    "                summary = {\n",
    "                    'Creator': creator_num,\n",
    "                    'Strategy': strategy,\n",
    "                    'Metric': f'{metric_name} Reference',\n",
    "                    'Value': df['Reference_MTLD'].median()\n",
    "                }\n",
    "                summary_data.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Create plots\n",
    "for idx, (metric_key, metric_name) in enumerate(metrics.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if metric_name == 'MTLD':\n",
    "        # For MTLD, plot both candidate and reference side by side\n",
    "        plot_data = summary_df[\n",
    "            (summary_df['Metric'].isin([metric_name, f'{metric_name} Reference']))\n",
    "        ]\n",
    "        \n",
    "        sns.barplot(data=plot_data,\n",
    "                   x='Strategy', y='Value', hue='Creator',\n",
    "                   ax=ax, ci=None)  # Remove confidence intervals\n",
    "        \n",
    "        # Add pattern to distinguish reference bars\n",
    "        for i, bar in enumerate(ax.patches):\n",
    "            if i >= len(ax.patches)/2:  # Second half of bars are reference\n",
    "                bar.set_hatch('//')\n",
    "                bar.set_alpha(0.7)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}',\n",
    "                   ha='center', va='bottom')\n",
    "    else:\n",
    "        # For other metrics, plot only candidate values\n",
    "        plot_data = summary_df[summary_df['Metric'] == metric_name]\n",
    "        sns.barplot(data=plot_data,\n",
    "                   x='Strategy', y='Value', hue='Creator',\n",
    "                   ax=ax)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(ax.patches):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}',\n",
    "                   ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_title(metric_name)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Median Value')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Only keep one legend (on the rightmost plot)\n",
    "    if idx < 2:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical summary\n",
    "print(\"\\nNumerical Summary:\")\n",
    "summary_pivot = summary_df.pivot_table(\n",
    "    values='Value',\n",
    "    index=['Creator', 'Strategy'],\n",
    "    columns=['Metric'],\n",
    "    aggfunc='first'\n",
    ").round(4)\n",
    "print(summary_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.1: New Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_comprehensive_statistics():\n",
    "    metrics = ['BERTScore_F1', 'Syntactic_Similarity', 'Candidate_MTLD', 'Candidate_Sentiment_Compound']\n",
    "    pretrained_df = pd.read_csv('#path')\n",
    "    \n",
    "    with open(\"comprehensive_statistics.txt\", 'w') as f:\n",
    "        # 1. RQ1: Pretrained vs Fine-tuned (First Creator, all strategies)\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"RQ1: PRETRAINED VS FINE-TUNED (First Creator)\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for metric in metrics:\n",
    "            f.write(f\"\\n{metric} Statistics:\\n{'-'*50}\\n\")\n",
    "            \n",
    "            # Pretrained stats\n",
    "            pretrained_values = pretrained_df[metric].dropna()\n",
    "            f.write(\"\\nPretrained Model:\\n\")\n",
    "            f.write(f\"Mean: {pretrained_values.mean():.4f}\\n\")\n",
    "            f.write(f\"Median: {pretrained_values.median():.4f}\\n\")\n",
    "            f.write(f\"Std: {pretrained_values.std():.4f}\\n\")\n",
    "            f.write(f\"Q1: {pretrained_values.quantile(0.25):.4f}\\n\")\n",
    "            f.write(f\"Q3: {pretrained_values.quantile(0.75):.4f}\\n\")\n",
    "            \n",
    "            # Stats for all three strategies\n",
    "            for strategy in ['Strategy 1', 'Strategy 2', 'Strategy 3']:\n",
    "                strategy_values = results['creator1'][strategy][metric].dropna()\n",
    "                f.write(f\"\\n{strategy}:\\n\")\n",
    "                f.write(f\"Mean: {strategy_values.mean():.4f}\\n\")\n",
    "                f.write(f\"Median: {strategy_values.median():.4f}\\n\")\n",
    "                f.write(f\"Std: {strategy_values.std():.4f}\\n\")\n",
    "                f.write(f\"Q1: {strategy_values.quantile(0.25):.4f}\\n\")\n",
    "                f.write(f\"Q3: {strategy_values.quantile(0.75):.4f}\\n\")\n",
    "                \n",
    "                # Statistical tests\n",
    "                stat, pval = stats.mannwhitneyu(pretrained_values, strategy_values)\n",
    "                d = cohen_d(pretrained_values, strategy_values)\n",
    "                f.write(f\"Mann-Whitney U test p-value vs Pretrained: {pval:.4f}\\n\")\n",
    "                f.write(f\"Cohen's d effect size vs Pretrained: {d:.4f}\\n\")\n",
    "            \n",
    "            # Overall comparison\n",
    "            all_values = [pretrained_values] + [results['creator1'][s][metric].dropna() \n",
    "                                              for s in ['Strategy 1', 'Strategy 2', 'Strategy 3']]\n",
    "            h_stat, p_val = stats.kruskal(*all_values)\n",
    "            f.write(f\"\\nKruskal-Wallis H-test p-value (all models): {p_val:.4f}\\n\")\n",
    "        \n",
    "        # 2. Fine-tuned Strategies Comparison (rest of the analysis remains the same)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"FINE-TUNED STRATEGIES COMPARISON\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # [Rest of the code remains the same]\n",
    "\n",
    "# Run the analysis\n",
    "compute_comprehensive_statistics()\n",
    "print(\"Statistics have been saved to 'comprehensive_statistics.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enhanced_statistics():\n",
    "    metrics = ['BERTScore_F1', 'Syntactic_Similarity', 'Candidate_MTLD', 'Candidate_Sentiment_Compound']\n",
    "    pretrained_df = pd.read_csv('#path')\n",
    "    \n",
    "    with open(\"enhanced_statistics.txt\", 'w') as f:\n",
    "        # 1. RQ1: Pretrained vs All Fine-tuned Strategies (Creator 1)\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"RQ1: PRETRAINED VS ALL FINE-TUNED STRATEGIES (Creator 1)\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for metric in metrics:\n",
    "            f.write(f\"\\n{metric} Statistics:\\n{'-'*50}\\n\")\n",
    "            \n",
    "            # Pretrained stats\n",
    "            pretrained_values = pretrained_df[metric].dropna()\n",
    "            ci = stats.t.interval(confidence=0.95,\n",
    "                                df=len(pretrained_values)-1,\n",
    "                                loc=pretrained_values.mean(),\n",
    "                                scale=stats.sem(pretrained_values))\n",
    "            \n",
    "            f.write(\"\\nPretrained Model:\\n\")\n",
    "            f.write(f\"Mean: {pretrained_values.mean():.4f}\\n\")\n",
    "            f.write(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\\n\")\n",
    "            f.write(f\"Median: {pretrained_values.median():.4f}\\n\")\n",
    "            f.write(f\"Std: {pretrained_values.std():.4f}\\n\")\n",
    "            f.write(f\"Q1: {pretrained_values.quantile(0.25):.4f}\\n\")\n",
    "            f.write(f\"Q3: {pretrained_values.quantile(0.75):.4f}\\n\")\n",
    "            \n",
    "            # Stats for all three strategies\n",
    "            for strategy in ['Strategy 1', 'Strategy 2', 'Strategy 3']:\n",
    "                strategy_values = results['creator1'][strategy][metric].dropna()\n",
    "                ci = stats.t.interval(confidence=0.95,\n",
    "                                    df=len(strategy_values)-1,\n",
    "                                    loc=strategy_values.mean(),\n",
    "                                    scale=stats.sem(strategy_values))\n",
    "                \n",
    "                f.write(f\"\\n{strategy}:\\n\")\n",
    "                f.write(f\"Mean: {strategy_values.mean():.4f}\\n\")\n",
    "                f.write(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\\n\")\n",
    "                f.write(f\"Median: {strategy_values.median():.4f}\\n\")\n",
    "                f.write(f\"Std: {strategy_values.std():.4f}\\n\")\n",
    "                f.write(f\"Q1: {strategy_values.quantile(0.25):.4f}\\n\")\n",
    "                f.write(f\"Q3: {strategy_values.quantile(0.75):.4f}\\n\")\n",
    "                \n",
    "                # Effect sizes and statistical tests against pretrained\n",
    "                d = cohen_d(pretrained_values, strategy_values)\n",
    "                stat, pval = stats.mannwhitneyu(pretrained_values, strategy_values)\n",
    "                \n",
    "                f.write(f\"\\nComparison with Pretrained:\\n\")\n",
    "                f.write(f\"Cohen's d: {d:.4f}\\n\")\n",
    "                f.write(f\"Mann-Whitney U test p-value: {pval:.4f}\\n\")\n",
    "            \n",
    "            # Kruskal-Wallis test across all models (pretrained + 3 strategies)\n",
    "            all_values = [pretrained_values] + [results['creator1'][s][metric].dropna() \n",
    "                                              for s in ['Strategy 1', 'Strategy 2', 'Strategy 3']]\n",
    "            h_stat, p_val = stats.kruskal(*all_values)\n",
    "            n = sum(len(x) for x in all_values)\n",
    "            eta_sq = (h_stat - len(all_values) + 1) / (n - len(all_values))\n",
    "            \n",
    "            f.write(f\"\\nOverall Comparison:\\n\")\n",
    "            f.write(f\"Kruskal-Wallis H-test p-value: {p_val:.4f}\\n\")\n",
    "            f.write(f\"Eta-squared effect size: {eta_sq:.4f}\\n\")\n",
    "            \n",
    "        # [Rest of the code remains the same for correlation analysis and fine-tuned comparisons]\n",
    "\n",
    "# Run enhanced analysis\n",
    "compute_enhanced_statistics()\n",
    "print(\"Enhanced statistics have been saved to 'enhanced_statistics.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reference_inclusive_statistics():\n",
    "    metrics = {\n",
    "        'MTLD': {'reference': 'Reference_MTLD', 'candidate': 'Candidate_MTLD'},\n",
    "        'Sentiment': {'reference': 'Reference_Sentiment_Compound', 'candidate': 'Candidate_Sentiment_Compound'}\n",
    "    }\n",
    "    \n",
    "    pretrained_df = pd.read_csv('#path')\n",
    "    \n",
    "    with open(\"reference_inclusive_statistics.txt\", 'w') as f:\n",
    "        # RQ1: Reference vs Pretrained vs Fine-tuned\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"RQ1: REFERENCE VS PRETRAINED VS FINE-TUNED COMPARISON\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for metric_name, metric_keys in metrics.items():\n",
    "            f.write(f\"\\n{metric_name} Statistics:\\n{'-'*50}\\n\")\n",
    "            \n",
    "            # Get reference values (from Strategy 1 of first creator)\n",
    "            reference_values = results['creator1']['Strategy 1'][metric_keys['reference']].dropna()\n",
    "            \n",
    "            # Reference stats\n",
    "            ci = stats.t.interval(confidence=0.95,\n",
    "                                df=len(reference_values)-1,\n",
    "                                loc=reference_values.mean(),\n",
    "                                scale=stats.sem(reference_values))\n",
    "            \n",
    "            f.write(\"\\nReference Values:\\n\")\n",
    "            f.write(f\"Mean: {reference_values.mean():.4f}\\n\")\n",
    "            f.write(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\\n\")\n",
    "            f.write(f\"Median: {reference_values.median():.4f}\\n\")\n",
    "            f.write(f\"Std: {reference_values.std():.4f}\\n\")\n",
    "            f.write(f\"Q1: {reference_values.quantile(0.25):.4f}\\n\")\n",
    "            f.write(f\"Q3: {reference_values.quantile(0.75):.4f}\\n\")\n",
    "            \n",
    "            # Pretrained stats\n",
    "            pretrained_values = pretrained_df[metric_keys['candidate']].dropna()\n",
    "            ci = stats.t.interval(confidence=0.95,\n",
    "                                df=len(pretrained_values)-1,\n",
    "                                loc=pretrained_values.mean(),\n",
    "                                scale=stats.sem(pretrained_values))\n",
    "            \n",
    "            f.write(\"\\nPretrained Model:\\n\")\n",
    "            f.write(f\"Mean: {pretrained_values.mean():.4f}\\n\")\n",
    "            f.write(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\\n\")\n",
    "            f.write(f\"Median: {pretrained_values.median():.4f}\\n\")\n",
    "            f.write(f\"Std: {pretrained_values.std():.4f}\\n\")\n",
    "            f.write(f\"Q1: {pretrained_values.quantile(0.25):.4f}\\n\")\n",
    "            f.write(f\"Q3: {pretrained_values.quantile(0.75):.4f}\\n\")\n",
    "            \n",
    "            # Compare with reference\n",
    "            d_ref_pre = cohen_d(reference_values, pretrained_values)\n",
    "            stat, pval = stats.mannwhitneyu(reference_values, pretrained_values)\n",
    "            f.write(f\"\\nComparison with Reference:\\n\")\n",
    "            f.write(f\"Cohen's d: {d_ref_pre:.4f}\\n\")\n",
    "            f.write(f\"Mann-Whitney U test p-value: {pval:.4f}\\n\")\n",
    "            \n",
    "            # Stats for all three strategies\n",
    "            for strategy in ['Strategy 1', 'Strategy 2', 'Strategy 3']:\n",
    "                strategy_values = results['creator1'][strategy][metric_keys['candidate']].dropna()\n",
    "                ci = stats.t.interval(confidence=0.95,\n",
    "                                    df=len(strategy_values)-1,\n",
    "                                    loc=strategy_values.mean(),\n",
    "                                    scale=stats.sem(strategy_values))\n",
    "                \n",
    "                f.write(f\"\\n{strategy}:\\n\")\n",
    "                f.write(f\"Mean: {strategy_values.mean():.4f}\\n\")\n",
    "                f.write(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\\n\")\n",
    "                f.write(f\"Median: {strategy_values.median():.4f}\\n\")\n",
    "                f.write(f\"Std: {strategy_values.std():.4f}\\n\")\n",
    "                f.write(f\"Q1: {strategy_values.quantile(0.25):.4f}\\n\")\n",
    "                f.write(f\"Q3: {strategy_values.quantile(0.75):.4f}\\n\")\n",
    "                \n",
    "                # Compare with reference and pretrained\n",
    "                d_ref = cohen_d(reference_values, strategy_values)\n",
    "                d_pre = cohen_d(pretrained_values, strategy_values)\n",
    "                stat_ref, pval_ref = stats.mannwhitneyu(reference_values, strategy_values)\n",
    "                stat_pre, pval_pre = stats.mannwhitneyu(pretrained_values, strategy_values)\n",
    "                \n",
    "                f.write(f\"\\nComparisons:\\n\")\n",
    "                f.write(f\"vs Reference - Cohen's d: {d_ref:.4f}, p-value: {pval_ref:.4f}\\n\")\n",
    "                f.write(f\"vs Pretrained - Cohen's d: {d_pre:.4f}, p-value: {pval_pre:.4f}\\n\")\n",
    "            \n",
    "            # Overall comparison including reference\n",
    "            all_values = [reference_values, pretrained_values] + [\n",
    "                results['creator1'][s][metric_keys['candidate']].dropna() \n",
    "                for s in ['Strategy 1', 'Strategy 2', 'Strategy 3']\n",
    "            ]\n",
    "            h_stat, p_val = stats.kruskal(*all_values)\n",
    "            n = sum(len(x) for x in all_values)\n",
    "            eta_sq = (h_stat - len(all_values) + 1) / (n - len(all_values))\n",
    "            \n",
    "            f.write(f\"\\nOverall Comparison:\\n\")\n",
    "            f.write(f\"Kruskal-Wallis H-test p-value: {p_val:.4f}\\n\")\n",
    "            f.write(f\"Eta-squared effect size: {eta_sq:.4f}\\n\")\n",
    "        \n",
    "        # RQ2: Fine-tuned Strategies Comparison (including reference)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"RQ2: FINE-TUNED STRATEGIES COMPARISON (Including Reference)\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        for creator, creator_results in results.items():\n",
    "            f.write(f\"\\nCreator: {creator}\\n{'-'*50}\\n\")\n",
    "            \n",
    "            for metric_name, metric_keys in metrics.items():\n",
    "                f.write(f\"\\n{metric_name} Statistics:\\n\")\n",
    "                \n",
    "                # Get reference values for this creator\n",
    "                reference_values = creator_results['Strategy 1'][metric_keys['reference']].dropna()\n",
    "                \n",
    "                # Prepare stats including reference\n",
    "                stats_dict = {'Reference': {\n",
    "                    'mean': reference_values.mean(),\n",
    "                    'median': reference_values.median(),\n",
    "                    'std': reference_values.std(),\n",
    "                    'q1': reference_values.quantile(0.25),\n",
    "                    'q3': reference_values.quantile(0.75)\n",
    "                }}\n",
    "                \n",
    "                for strategy, df in creator_results.items():\n",
    "                    values = df[metric_keys['candidate']].dropna()\n",
    "                    stats_dict[strategy] = {\n",
    "                        'mean': values.mean(),\n",
    "                        'median': values.median(),\n",
    "                        'std': values.std(),\n",
    "                        'q1': values.quantile(0.25),\n",
    "                        'q3': values.quantile(0.75)\n",
    "                    }\n",
    "                \n",
    "                stats_df = pd.DataFrame(stats_dict).round(4)\n",
    "                f.write(f\"\\n{stats_df.to_string()}\\n\")\n",
    "                \n",
    "                # Kruskal-Wallis including reference\n",
    "                strategy_values = [reference_values] + [df[metric_keys['candidate']].dropna() \n",
    "                                                      for df in creator_results.values()]\n",
    "                h_stat, p_val = stats.kruskal(*strategy_values)\n",
    "                n = sum(len(x) for x in strategy_values)\n",
    "                eta_sq = (h_stat - len(strategy_values) + 1) / (n - len(strategy_values))\n",
    "                \n",
    "                f.write(f\"\\nKruskal-Wallis H-test p-value: {p_val:.4f}\\n\")\n",
    "                f.write(f\"Eta-squared effect size: {eta_sq:.4f}\\n\")\n",
    "\n",
    "# Run the analysis\n",
    "compute_reference_inclusive_statistics()\n",
    "print(\"Reference-inclusive statistics have been saved to 'reference_inclusive_statistics.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
