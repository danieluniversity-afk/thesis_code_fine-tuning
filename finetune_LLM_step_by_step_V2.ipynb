{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each cell that I could, I added some code to save some statistics in a different file so that you dont have to scroll through this notebook for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 & 1: Understanding and Cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn \n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To run, update the second cell with a filepath and a name and RUN ALL!  \n",
    " (you need a dataset with similar shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Setup and Initialization\n",
    "\n",
    "# Create base output directory\n",
    "os.makedirs('output_files', exist_ok=True)\n",
    "\n",
    "# Define input data structure (example format)\n",
    "CREATORS = []  # Add creator names here\n",
    "FILEPATHS = [\n",
    "                # add filepaths here\n",
    "    ]\n",
    "\n",
    "# Create creator folders\n",
    "for creator in CREATORS:\n",
    "    os.makedirs(f'output_files/{creator}', exist_ok=True)\n",
    "\n",
    "# Create DataFrames dictionary to store each creator's data\n",
    "dfs = {}\n",
    "\n",
    "# Create/open the statistics file\n",
    "with open('output_files/statistics.txt', 'w') as f:\n",
    "    f.write(\"Initial Statistics:\\n\\n\")\n",
    "\n",
    "# Load all datasets\n",
    "for creator, path in zip(CREATORS, FILEPATHS):\n",
    "    try:\n",
    "        # Modified this line to properly handle Windows paths\n",
    "        dfs[creator] = pd.read_csv(rf'{path}')\n",
    "        print(f\"Successfully loaded dataset for {creator}\")\n",
    "        \n",
    "        # Append initial statistics to the main statistics file\n",
    "        with open('output_files/statistics.txt', 'a') as f:\n",
    "            f.write(f\"{creator} initial row count: {len(dfs[creator])}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset for {creator}: {e}\")\n",
    "\n",
    "# Print initial statistics for each dataset\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\nStatistics for {creator}:\")\n",
    "    print(f\"Number of rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns in each DataFrame\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== Columns in DataFrame for {creator} ===\")\n",
    "    print(df.columns.tolist())\n",
    "# Drop unnecesary collumns after checking for Duplicates - we ensure that we don't cause more dupes by keeping the id and the time they were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN's & Duplicates & Unique users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaNs and duplicates for each creator's dataset\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\nNaN and Duplicate Analysis:\\n\")\n",
    "    \n",
    "    for creator, df in dfs.items():\n",
    "        f.write(f\"\\n=== STATISTICS FOR {creator} ===\\n\")\n",
    "        \n",
    "        # Count unique senders and recipients\n",
    "        unique_senders = df['sender_handle'].nunique()\n",
    "        unique_recipients = df['recipient_handle'].nunique()\n",
    "        \n",
    "        f.write(f\"Number of unique senders: {unique_senders}\\n\")\n",
    "        f.write(f\"Number of unique recipients: {unique_recipients}\\n\")\n",
    "        \n",
    "        # Calculate NaN percentages\n",
    "        nan_percentage = (df.isna().sum() / len(df)) * 100\n",
    "        f.write(\"\\nPercentage of NaNs per column:\\n\")\n",
    "        for col, pct in nan_percentage.items():\n",
    "            f.write(f\"{col}: {pct:.2f}%\\n\")\n",
    "        \n",
    "        # Check duplicates\n",
    "        duplicates = df.duplicated().sum()\n",
    "        f.write(f\"\\nNumber of duplicate rows: {duplicates}\\n\")\n",
    "        \n",
    "        # Print to console as well\n",
    "        print(f\"\\n=== STATISTICS FOR {creator} ===\")\n",
    "        print(f\"Number of unique senders: {unique_senders}\")\n",
    "        print(f\"Number of unique recipients: {unique_recipients}\")\n",
    "        print(\"\\nPercentage of NaNs per column:\")\n",
    "        print(nan_percentage)\n",
    "        print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "        \n",
    "        if duplicates > 0:\n",
    "            duplicate_rows = df[df.duplicated(keep=False)]\n",
    "            print(\"\\nDuplicate rows:\")\n",
    "            print(duplicate_rows)\n",
    "            # Optionally we could save duplicate rows to the statistics file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hmm checking which and how many rows are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check empty text rows for each creator's dataset\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\nEmpty Text Analysis:\\n\")\n",
    "    \n",
    "    for creator, df in dfs.items():\n",
    "        empty_text_count = df['text'].isna().sum()\n",
    "        empty_text_rows = df[df['text'].isna()]\n",
    "        \n",
    "        # Write count to statistics file\n",
    "        f.write(f\"\\n=== {creator} ===\\n\")\n",
    "        f.write(f\"Number of rows with empty text: {empty_text_count}\\n\")\n",
    "        \n",
    "        # Print to console\n",
    "        print(f\"\\n=== {creator} ===\")\n",
    "        print(f\"Number of rows with empty text: {empty_text_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE have the numbers of duplicates now. Our focus is on 3 columns; text, sender_handle and recipient_handle. We are ignoring the created_at knowing that the data is ordered in ascending order.\n",
    "### At this point the duplicates should be dropped (I didn't right away knowing that it will drop later when filtering the data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More UNDERSTANDING: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sender and recipient activity for each creator\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== Analysis for {creator} ===\")\n",
    "    \n",
    "    # Count how many times each sender has sent a message\n",
    "    sender_counts = df['sender_handle'].value_counts().reset_index()\n",
    "    sender_counts.columns = ['Sender', 'Messages Sent']\n",
    "\n",
    "    # Count how many times each recipient has received a message\n",
    "    recipient_counts = df['recipient_handle'].value_counts().reset_index()\n",
    "    recipient_counts.columns = ['Recipient', 'Messages Received']\n",
    "\n",
    "    # Create a DataFrame to display counts for Creator as a sender and recipient\n",
    "    creator_as_sender = sender_counts[sender_counts['Sender'] == creator]\n",
    "    creator_as_recipient = recipient_counts[recipient_counts['Recipient'] == creator]\n",
    "\n",
    "    # Combine Creator's data with overall data for easy comparison\n",
    "    combined_counts = pd.DataFrame({\n",
    "        'Role': [f'{creator} as Sender', f'{creator} as Recipient'],\n",
    "        'Count': [creator_as_sender.iloc[0]['Messages Sent'] if not creator_as_sender.empty else 0,\n",
    "                creator_as_recipient.iloc[0]['Messages Received'] if not creator_as_recipient.empty else 0]\n",
    "    })\n",
    "\n",
    "    # Append other unique senders and recipients, excluding Creator for clarity\n",
    "    other_senders = sender_counts[sender_counts['Sender'] != creator]\n",
    "    other_recipients = recipient_counts[recipient_counts['Recipient'] != creator]\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"\\n{creator}'s Activity:\")\n",
    "    print(combined_counts)\n",
    "    print(\"\\nOther Senders' Activity:\")\n",
    "    print(other_senders.head(2))\n",
    "    print(\"\\nOther Recipients' Activity:\")\n",
    "    print(other_recipients.head(2))\n",
    "    \n",
    "    # Save combined counts to statistics file\n",
    "    with open('output_files/statistics.txt', 'a') as f:\n",
    "        f.write(f\"\\n=== Activity Analysis for {creator} ===\\n\")\n",
    "        f.write(f\"{creator}'s Activity:\\n\")\n",
    "        f.write(combined_counts.to_string())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Story: For what it's worth, during preliminary tests I had a hard time combining the 2 dataframes into a single summary csv. In the end, i just processed things differently so that the data is all in one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking rows where the 'creator' is neither the sender nor the recipient (just to be safe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where creator is neither sender nor recipient\n",
    "for creator, df in dfs.items():\n",
    "    rows_without_creator = df[(df['sender_handle'] != creator) & (df['recipient_handle'] != creator)]\n",
    "    print(f\"\\n=== {creator} ===\")\n",
    "    print(f\"Number of rows without {creator} as sender or recipient: {rows_without_creator.shape[0]}\")\n",
    "    \n",
    "    # Save to statistics file\n",
    "    with open('output_files/statistics.txt', 'a') as f:\n",
    "        f.write(f\"\\nRows without {creator}:\\n\")\n",
    "        f.write(f\"Number of rows without {creator} as sender or recipient: {rows_without_creator.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if each sender is also a recipient and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sender/recipient overlap for each creator\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== {creator} ===\")\n",
    "    \n",
    "    # Create sets of all unique senders and recipients\n",
    "    unique_senders = set(df['sender_handle'])\n",
    "    unique_recipients = set(df['recipient_handle'])\n",
    "\n",
    "    # Check if each sender is at least once a recipient\n",
    "    senders_not_recipients = unique_senders.difference(unique_recipients)\n",
    "    # Check if each recipient is at least once a sender\n",
    "    recipients_not_senders = unique_recipients.difference(unique_senders)\n",
    "\n",
    "    # Print out the results\n",
    "    print(\"Number of senders who never received a message:\", len(senders_not_recipients))\n",
    "    print(\"Number of recipients who never sent a message:\", len(recipients_not_senders))\n",
    "    \n",
    "    # Save to statistics file\n",
    "    with open('output_files/statistics.txt', 'a') as f:\n",
    "        f.write(f\"\\nSender/Recipient Analysis for {creator}:\\n\")\n",
    "        f.write(f\"Number of senders who never received a message: {len(senders_not_recipients)}\\n\")\n",
    "        f.write(f\"Number of recipients who never sent a message: {len(recipients_not_senders)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop each unique_person that didn't send or recieve a message (because you don't have a conversation) \n",
    "#### Note: I didn't drop them immedietly - I will filter everything in one go (so also the duplicates and NaN rows).\n",
    "\n",
    "### Although we already have the number of unique senders and recipients - to determine the number of BOTH a SENDER & RECIPIENT, a simple intersection can be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check individuals who are both senders and recipients for each creator\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\nSender/Recipient Intersection Analysis:\\n\")\n",
    "    \n",
    "    for creator, df in dfs.items():\n",
    "        # Create sets of all unique senders and recipients\n",
    "        unique_senders = set(df['sender_handle'])\n",
    "        unique_recipients = set(df['recipient_handle'])\n",
    "        \n",
    "        # Find individuals who are both senders and recipients using set intersection\n",
    "        both_senders_and_recipients = unique_senders.intersection(unique_recipients)\n",
    "        \n",
    "        # Save and print results\n",
    "        f.write(f\"\\n=== {creator} ===\\n\")\n",
    "        f.write(f\"Number of unique individuals who are both senders and recipients: {len(both_senders_and_recipients)}\\n\")\n",
    "        \n",
    "        print(f\"\\n=== {creator} ===\")\n",
    "        print(f\"Number of unique individuals who are both senders and recipients: {len(both_senders_and_recipients)}\")\n",
    "        \n",
    "        # Uncomment this if you want to also see the list\n",
    "        # print(\"List of individuals who are both senders and recipients:\")\n",
    "        # for person in both_senders_and_recipients:\n",
    "        #     print(person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now dropfilter the following: 1) Duplicates 2) NaNs rows from text column 3) Unique people that are not both senders and recipients 4) unique users that didn't send AND recieve more than 3 messages. \n",
    "We want to train on actual chats so small conversations (under a few messages) would add noise to the data and we already have more than enough.\n",
    "Also at this point the mass messages could be dropped - but preliminary tests showed that they are not a problem (not that many and they might get dropped now anyway - can also be dropped later). Mass messages are when the creator sends to all users at once the same message. Another reason for keeping such rows (even if it would be automatic send) is that people repsond to it so we want the model to understand the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and filter each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== Processing {creator}'s dataset ===\")\n",
    "    \n",
    "    # Store initial number of rows\n",
    "    initial_rows = len(df)\n",
    "    print(f\"Initial number of rows: {initial_rows}\")\n",
    "    \n",
    "    # First drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Rows after dropping duplicates: {len(df)}\")\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    # df = df.drop(['id', 'created_at'], axis=1)\n",
    "    # print(\"Dropped 'id' and 'created_at' columns\")\n",
    "    \n",
    "    # Drop NaN rows from text column\n",
    "    df = df.dropna(subset=['text'])\n",
    "    print(f\"Rows after dropping NaN: {len(df)}\")\n",
    "    \n",
    "    # Get the intersection set for this creator's dataset\n",
    "    unique_senders = set(df['sender_handle'])\n",
    "    unique_recipients = set(df['recipient_handle'])\n",
    "    both_senders_and_recipients = unique_senders.intersection(unique_recipients)\n",
    "    \n",
    "    # Filter rows where both sender and recipient are in the intersection set\n",
    "    df = df[df['sender_handle'].isin(both_senders_and_recipients) & \n",
    "            df['recipient_handle'].isin(both_senders_and_recipients)]\n",
    "    print(f\"Rows after filtering non-conversational users: {len(df)}\")\n",
    "    \n",
    "    # Count messages sent and received by each person\n",
    "    send_counts = df['sender_handle'].value_counts()\n",
    "    receive_counts = df['recipient_handle'].value_counts()\n",
    "    \n",
    "    # Combine send and receive counts\n",
    "    counts = pd.DataFrame({\n",
    "        'sent': send_counts,\n",
    "        'received': receive_counts\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Keep only users who have sent AND received more than 3 messages\n",
    "    qualified_users = counts[(counts['sent'] > 3) & (counts['received'] > 3)].index\n",
    "    \n",
    "    # Final filtering to keep only qualified users\n",
    "    filtered_df = df[df['sender_handle'].isin(qualified_users) & \n",
    "                    df['recipient_handle'].isin(qualified_users)]\n",
    "    \n",
    "    print(f\"Final number of rows: {len(filtered_df)}\")\n",
    "    \n",
    "    # Save filtered data\n",
    "    output_path = f'output_files/{creator}/filtered_data.csv'\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "    print(f\"Filtered data saved to {output_path}\")\n",
    "    print(f\"Remaining participants: {len(qualified_users)}\")\n",
    "    \n",
    "    # Update the dictionary with filtered data\n",
    "    dfs[creator] = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some EDA!\n",
    "#### ! Before taking a look at the data lets talk about its current shape. Messages are in chronological order already but not ordered by person (eg. if 3 people texted the creator at the same time, the messages will be one after another, instead of being separated by conversation/unique_person).\n",
    "\n",
    "#### Lets visualise/understand what we have! Excluding the creator because it has over way more messags than the most active fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze message counts for each creator\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== Analysis for {creator} ===\")\n",
    "    \n",
    "    # Count how many times each person has sent and received a message after cleaning.\n",
    "    messages_sent_counts = df['sender_handle'].value_counts().reset_index()\n",
    "    messages_sent_counts.columns = ['unique_person', 'messages_sent']\n",
    "    messages_received_counts = df['recipient_handle'].value_counts().reset_index()\n",
    "    messages_received_counts.columns = ['unique_person', 'messages_received']\n",
    "\n",
    "    # Convert the series into a DataFrame for easier manipulation and future visualizations\n",
    "    df_messages_sent = pd.DataFrame(messages_sent_counts)\n",
    "    df_messages_received = pd.DataFrame(messages_received_counts)\n",
    "\n",
    "    # Get creator's statistics\n",
    "    creator_sent = df_messages_sent[df_messages_sent['unique_person'] == creator]['messages_sent'].iloc[0]\n",
    "    creator_received = df_messages_received[df_messages_received['unique_person'] == creator]['messages_received'].iloc[0]\n",
    "    \n",
    "    # Append to statistics file\n",
    "    with open('output_files/statistics.txt', 'a') as f:\n",
    "        f.write(f\"\\nCreator Activity Statistics for {creator}:\\n\")\n",
    "        f.write(f\"Messages sent by creator: {creator_sent}\\n\")\n",
    "        f.write(f\"Messages received by creator: {creator_received}\\n\")\n",
    "        f.write(f\"Total messages involving creator: {creator_sent + creator_received}\\n\\n\")\n",
    "\n",
    "    print(\"DataFrame of Messages Sent Counts:\")\n",
    "    print(df_messages_sent.head(2))\n",
    "    print(\"\\nDataFrame of Messages Received Counts:\") \n",
    "    print(df_messages_received.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (creator, df) in enumerate(dfs.items(), 1):\n",
    "    # Create message count DataFrames for this creator\n",
    "    messages_sent_counts = df['sender_handle'].value_counts().reset_index()\n",
    "    messages_sent_counts.columns = ['unique_person', 'messages_sent']\n",
    "    messages_received_counts = df['recipient_handle'].value_counts().reset_index()\n",
    "    messages_received_counts.columns = ['unique_person', 'messages_received']\n",
    "    \n",
    "    # Create DataFrames and exclude the creator\n",
    "    df_messages_sent = messages_sent_counts[messages_sent_counts['unique_person'] != creator]\n",
    "    df_messages_received = messages_received_counts[messages_received_counts['unique_person'] != creator]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    fig.suptitle(f'Message Distribution Analysis (Excluding Creator {i})', fontsize=14)\n",
    "    \n",
    "    # Plot messages sent\n",
    "    ax1.boxplot(df_messages_sent['messages_sent'], vert=False)\n",
    "    ax1.set_title('Distribution of Messages Sent')\n",
    "    ax1.set_xlabel('Number of Messages')\n",
    "    \n",
    "    # Plot messages received\n",
    "    ax2.boxplot(df_messages_received['messages_received'], vert=False)\n",
    "    ax2.set_title('Distribution of Messages Received')\n",
    "    ax2.set_xlabel('Number of Messages')\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure to creator's folder\n",
    "    plt.savefig(f'output_files/{creator}/box_plot_message_distribution_with_outliers.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without the outlier now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (creator, df) in enumerate(dfs.items(), 1):\n",
    "    # Create message count DataFrames for this creator\n",
    "    messages_sent_counts = df['sender_handle'].value_counts().reset_index()\n",
    "    messages_sent_counts.columns = ['unique_person', 'messages_sent']\n",
    "    messages_received_counts = df['recipient_handle'].value_counts().reset_index()\n",
    "    messages_received_counts.columns = ['unique_person', 'messages_received']\n",
    "    \n",
    "    # Create DataFrames and exclude the creator\n",
    "    df_messages_sent = messages_sent_counts[messages_sent_counts['unique_person'] != creator]\n",
    "    df_messages_received = messages_received_counts[messages_received_counts['unique_person'] != creator]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    fig.suptitle(f'Message Distribution Analysis (Excluding Creator {i})', fontsize=14)\n",
    "    \n",
    "    # Plot messages sent without outliers\n",
    "    ax1.boxplot(df_messages_sent['messages_sent'], vert=False, showfliers=False)\n",
    "    ax1.set_title('Distribution of Messages Sent')\n",
    "    ax1.set_xlabel('Number of Messages')\n",
    "    \n",
    "    # Plot messages received without outliers\n",
    "    ax2.boxplot(df_messages_received['messages_received'], vert=False, showfliers=False)\n",
    "    ax2.set_title('Distribution of Messages Received')\n",
    "    ax2.set_xlabel('Number of Messages')\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Let's check the distribution of messages and some extra statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for creator, df in dfs.items():\n",
    "    # Create message count DataFrames\n",
    "    messages_sent_counts = df['sender_handle'].value_counts().reset_index()\n",
    "    messages_sent_counts.columns = ['unique_person', 'messages_sent']\n",
    "    messages_received_counts = df['recipient_handle'].value_counts().reset_index()\n",
    "    messages_received_counts.columns = ['unique_person', 'messages_received']\n",
    "    \n",
    "    # Filter out creator\n",
    "    df_messages_sent_filtered = messages_sent_counts[messages_sent_counts['unique_person'] != creator]\n",
    "    df_messages_received_filtered = messages_received_counts[messages_received_counts['unique_person'] != creator]\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\n=== Distribution Statistics for {creator} ===\")\n",
    "    \n",
    "    print(\"\\n=== Messages Sent Distribution Info ===\")\n",
    "    print(f\"Total unique people: {len(df_messages_sent_filtered)}\")\n",
    "    print(f\"Total messages sent: {df_messages_sent_filtered['messages_sent'].sum()}\")\n",
    "    print(f\"Average messages per person: {df_messages_sent_filtered['messages_sent'].mean():.2f}\")\n",
    "    print(f\"Median messages per person: {df_messages_sent_filtered['messages_sent'].median():.2f}\")\n",
    "    print(f\"Maximum messages sent by one person: {df_messages_sent_filtered['messages_sent'].max()}\")\n",
    "\n",
    "    print(\"\\n=== Messages Received Distribution Info ===\")\n",
    "    print(f\"Total unique people: {len(df_messages_received_filtered)}\")\n",
    "    print(f\"Total messages received: {df_messages_received_filtered['messages_received'].sum()}\")\n",
    "    print(f\"Average messages per person: {df_messages_received_filtered['messages_received'].mean():.2f}\")\n",
    "    print(f\"Median messages per person: {df_messages_received_filtered['messages_received'].median():.2f}\")\n",
    "    print(f\"Maximum messages received by one person: {df_messages_received_filtered['messages_received'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save distribution statistics to the shared statistics file\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    for creator, df in dfs.items():\n",
    "        messages_sent_counts = df['sender_handle'].value_counts().reset_index()\n",
    "        messages_sent_counts.columns = ['unique_person', 'messages_sent']\n",
    "        messages_received_counts = df['recipient_handle'].value_counts().reset_index()\n",
    "        messages_received_counts.columns = ['unique_person', 'messages_received']\n",
    "        \n",
    "        df_messages_sent_filtered = messages_sent_counts[messages_sent_counts['unique_person'] != creator]\n",
    "        df_messages_received_filtered = messages_received_counts[messages_received_counts['unique_person'] != creator]\n",
    "        \n",
    "        f.write(f\"\\n=== Distribution Statistics for {creator} ===\\n\")\n",
    "        f.write(\"\\nMessages Sent Distribution Info:\\n\")\n",
    "        f.write(f\"Total unique people: {len(df_messages_sent_filtered)}\\n\")\n",
    "        f.write(f\"Total messages sent: {df_messages_sent_filtered['messages_sent'].sum()}\\n\")\n",
    "        f.write(f\"Average messages per person: {df_messages_sent_filtered['messages_sent'].mean():.2f}\\n\")\n",
    "        f.write(f\"Median messages per person: {df_messages_sent_filtered['messages_sent'].median():.2f}\\n\")\n",
    "        f.write(f\"Maximum messages sent by one person: {df_messages_sent_filtered['messages_sent'].max()}\\n\")\n",
    "        \n",
    "        f.write(\"\\nMessages Received Distribution Info:\\n\")\n",
    "        f.write(f\"Total unique people: {len(df_messages_received_filtered)}\\n\")\n",
    "        f.write(f\"Total messages received: {df_messages_received_filtered['messages_received'].sum()}\\n\")\n",
    "        f.write(f\"Average messages per person: {df_messages_received_filtered['messages_received'].mean():.2f}\\n\")\n",
    "        f.write(f\"Median messages per person: {df_messages_received_filtered['messages_received'].median():.2f}\\n\")\n",
    "        f.write(f\"Maximum messages received by one person: {df_messages_received_filtered['messages_received'].max()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Extracting, ordering, concatenating chats (per unique person) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down below is the script in case a specific chat is needed (from the whole dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST SAMPLE\n",
    "# file_path = r'filtered_data_guide.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "# desired_user = 'the_username_you_want'\n",
    "\n",
    "# user_df = df[(df['sender_handle'] == desired_user) | (df['recipient_handle'] == desired_user)]\n",
    "\n",
    "# new_file_path = f'chats_with_{desired_user}.csv'\n",
    "# user_df.to_csv(new_file_path, index=False)\n",
    "\n",
    "# print(f\"Data involving '{desired_user}' has been saved to {new_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see how many times CREATOR has interacted with someone:\n",
    "# desired_user = 'the_username_you_want'\n",
    "\n",
    "# creator_to_user = df[(df['sender_handle'] == CREATOR) & (df['recipient_handle'] == desired_user)].shape[0]\n",
    "# user_to_creator = df[(df['sender_handle'] == desired_user) & (df['recipient_handle'] == CREATOR)].shape[0]\n",
    "\n",
    "# print(f\"{CREATOR} texted {desired_user} {creator_to_user} times.\")\n",
    "# print(f\"{desired_user} texted {CREATOR} {user_to_creator} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order per person + Concatenate the msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique persons for each creator\n",
    "for creator, df in dfs.items():\n",
    "    # Combine sender and recipient handles into one Series and exclude creator\n",
    "    unique_persons = pd.concat([df['sender_handle'], df['recipient_handle']])\n",
    "    unique_persons = unique_persons[unique_persons != creator].unique()\n",
    "    \n",
    "    # Convert to list if explicitly needed as a list\n",
    "    unique_person_list = list(unique_persons)\n",
    "    \n",
    "    # Print the length of the list of unique persons excluding creator\n",
    "    print(f\"Number of unique persons excluding {creator}: {len(unique_person_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store ordered messages\n",
    "for creator, df in dfs.items():\n",
    "    ordered_messages = pd.DataFrame()\n",
    "    \n",
    "    # Get unique persons for this creator\n",
    "    unique_persons = pd.concat([df['sender_handle'], df['recipient_handle']])\n",
    "    unique_persons = unique_persons[unique_persons != creator].unique()\n",
    "    unique_person_list = list(unique_persons)\n",
    "    \n",
    "    # Iterate through each unique person and gather their messages\n",
    "    for person in unique_person_list:\n",
    "        person_messages = df[(df['sender_handle'] == person) | (df['recipient_handle'] == person)]\n",
    "        ordered_messages = pd.concat([ordered_messages, person_messages])\n",
    "\n",
    "    # Write the ordered messages to a new CSV file\n",
    "    output_path = f'output_files/{creator}/ordered_filtered_data.csv'\n",
    "    ordered_messages.to_csv(output_path, index=False)\n",
    "    print(f\"Ordered messages for {creator} have been saved to: {output_path}\")\n",
    "    \n",
    "    # Update the dictionary with ordered messages\n",
    "    dfs[creator] = ordered_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now have a csv odered by person AND in chronolgical order. Next step is to concatenate consecutive messages on same row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_messages(df):\n",
    "    results = []\n",
    "    current_messages = \"\"\n",
    "    current_sender = None\n",
    "    current_recipient = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Clean up the column names\n",
    "        current_text = str(row['text']).strip()\n",
    "\n",
    "        if current_sender is None or current_recipient is None:\n",
    "            # Start a new message block\n",
    "            current_sender = row['sender_handle']\n",
    "            current_recipient = row['recipient_handle']\n",
    "            current_messages = current_text\n",
    "        elif row['sender_handle'] == current_sender and row['recipient_handle'] == current_recipient:\n",
    "            # Continue concatenating messages for the same sender and recipient\n",
    "            current_messages += \" \" + current_text\n",
    "        else:\n",
    "            # Save the current messages before switching to a new sender or recipient\n",
    "            results.append({\n",
    "                'text': current_messages,\n",
    "                'sender_handle': current_sender,\n",
    "                'recipient_handle': current_recipient\n",
    "            })\n",
    "            # Reset for the next sender and recipient\n",
    "            current_sender = row['sender_handle']\n",
    "            current_recipient = row['recipient_handle']\n",
    "            current_messages = current_text\n",
    "\n",
    "    # Don't forget to add the last set of messages\n",
    "    if current_messages:\n",
    "        results.append({\n",
    "            'text': current_messages,\n",
    "            'sender_handle': current_sender,\n",
    "            'recipient_handle': current_recipient\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    # Make sure column names are stripped of leading/trailing whitespace and are lowercased\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    \n",
    "    # Process the data to concatenate messages\n",
    "    concatenated_df = concatenate_messages(df)\n",
    "    \n",
    "    # Save the concatenated messages to a new CSV\n",
    "    output_path = f'output_files/{creator}/concat_ordered_filtered_data.csv'\n",
    "    concatenated_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Update the dictionary with concatenated messages\n",
    "    dfs[creator] = concatenated_df\n",
    "    \n",
    "    print(f\"Concatenated conversations for {creator} have been saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows in concatenated files for each creator and save to statistics file\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\n\\nRows of data after cleaning:\\n\\n\")\n",
    "    for creator, df in dfs.items():\n",
    "        num_rows = len(df)\n",
    "        print(f\"Number of rows in {creator}'s concatenated file: {num_rows}\")\n",
    "        f.write(f\"{creator} rows after cleaning: {num_rows}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: SPLIT TRAIN-TEST (per random but fully conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_conversations(df, creator, target_size_percentage=5):\n",
    "    # Calculate target size\n",
    "    total_messages = len(df)\n",
    "    target_size = int(total_messages * (target_size_percentage / 100))\n",
    "    \n",
    "    # Get list of unique users who interacted with creator\n",
    "    users = set(df[df['sender_handle'] == creator]['recipient_handle']) | \\\n",
    "            set(df[df['recipient_handle'] == creator]['sender_handle'])\n",
    "    users = list(users)\n",
    "    \n",
    "    # Randomly shuffle users\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "    test_messages_count = 0\n",
    "    selected_users = []\n",
    "    \n",
    "    # Extract conversations until we reach target size\n",
    "    for user in users:\n",
    "        # Get all messages between creator and this user\n",
    "        user_convo = df[\n",
    "            ((df['sender_handle'] == creator) & (df['recipient_handle'] == user)) |\n",
    "            ((df['sender_handle'] == user) & (df['recipient_handle'] == creator))\n",
    "        ]\n",
    "        \n",
    "        test_messages_count += len(user_convo)\n",
    "        test_df = pd.concat([test_df, user_convo])\n",
    "        selected_users.append(user)\n",
    "        \n",
    "        if test_messages_count >= target_size:\n",
    "            break\n",
    "    \n",
    "    # Create training set (all remaining conversations)\n",
    "    train_df = df[~df.index.isin(test_df.index)]\n",
    "    \n",
    "    print(f\"Total messages: {total_messages}\")\n",
    "    print(f\"Test set messages: {len(test_df)} ({(len(test_df)/total_messages)*100:.2f}%)\")\n",
    "    print(f\"Training set messages: {len(train_df)} ({(len(train_df)/total_messages)*100:.2f}%)\")\n",
    "    print(f\"Selected users for test set: {len(selected_users)} out of {len(users)}\")\n",
    "    \n",
    "    return train_df, test_df, selected_users\n",
    "\n",
    "# Process each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== Processing {creator}'s dataset ===\")\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_df, test_df, selected_users = extract_test_conversations(df, creator, target_size_percentage=5)\n",
    "    \n",
    "    # Save the splits\n",
    "    train_path = f'output_files/{creator}/train_concat_ordered_filtered.csv'\n",
    "    test_path = f'output_files/{creator}/test_concat_ordered_filtered.csv'\n",
    "    users_path = f'output_files/{creator}/test_set_users.txt'\n",
    "    \n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    \n",
    "    # Save selected users for reference\n",
    "    with open(users_path, 'w') as f:\n",
    "        f.write('\\n'.join(selected_users))\n",
    "    \n",
    "    print(f\"Files saved to:\\n{train_path}\\n{test_path}\\n{users_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataset statistics before formatting for fine-tuning\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write('\\n\\nFinal Dataset Statistics After Train/Test Split:\\n\\n')\n",
    "    for creator, df in dfs.items():\n",
    "        train_df = pd.read_csv(f'output_files/{creator}/train_concat_ordered_filtered.csv')\n",
    "        test_df = pd.read_csv(f'output_files/{creator}/test_concat_ordered_filtered.csv')\n",
    "        \n",
    "        # Read selected test users\n",
    "        with open(f'output_files/{creator}/test_set_users.txt', 'r') as users_file:\n",
    "            test_users = users_file.read().splitlines()\n",
    "        \n",
    "        total_messages = len(train_df) + len(test_df)\n",
    "        \n",
    "        f.write(f'=== {creator} ===\\n')\n",
    "        f.write(f'Training set size: {len(train_df)} messages ({(len(train_df)/total_messages)*100:.2f}%)\\n')\n",
    "        f.write(f'Test set size: {len(test_df)} messages ({(len(test_df)/total_messages)*100:.2f}%)\\n')\n",
    "        f.write(f'Total: {total_messages} messages\\n')\n",
    "        f.write(f'Selected users for test set: {len(test_users)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this point, we have a clean, structured and organized dataset that we can format for fine-tuning in the required shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: JSONL (for fine-tuning) with Strategy Specific Format \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the format required for fine-tuning user-assistant pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also balancing after each strategy for more control and understanding. It could be done in one cell after all the strategies if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: One user-assistant pair per row. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantage: Will end up having more rows of training data.\n",
    "##### Disadvantage: No context given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_a_pairs(df, creator):\n",
    "    jsonl_pairs = []\n",
    "    \n",
    "    # Process each row\n",
    "    for i in range(len(df) - 1):  # Go up to second-to-last row\n",
    "        current_row = df.iloc[i]\n",
    "        next_row = df.iloc[i + 1]\n",
    "        \n",
    "        # Skip if current message is from assistant (creator)\n",
    "        if current_row['sender_handle'] == creator:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have a valid user-assistant pair\n",
    "        if (current_row['sender_handle'] != creator and \n",
    "            next_row['sender_handle'] == creator):\n",
    "            \n",
    "            pair = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": current_row['text']\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": next_row['text']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            jsonl_pairs.append(pair)\n",
    "    \n",
    "    return jsonl_pairs\n",
    "\n",
    "def save_to_jsonl(jsonl_pairs, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for pair in jsonl_pairs:\n",
    "            file.write(json.dumps(pair) + '\\n')\n",
    "\n",
    "# Process each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    # Load the training data\n",
    "    train_df = pd.read_csv(f'output_files/{creator}/train_concat_ordered_filtered.csv')\n",
    "    \n",
    "    # Generate the pairs and save to JSONL\n",
    "    pairs = generate_model_a_pairs(train_df, creator)\n",
    "    output_path = f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl'\n",
    "    save_to_jsonl(pairs, output_path)\n",
    "    \n",
    "    print(f\"Generated {len(pairs)} user-assistant pairs for {creator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some rules:\n",
    "{Skip any assistant messages at the start of conversations\n",
    "Automatically skip any user messages at the end of conversations (since they won't have an assistant response) }\n",
    "Because for the fine-tuning only user-assistant pair are accepted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_a_pairs(df, creator):\n",
    "    jsonl_pairs = []\n",
    "    \n",
    "    # Process each row\n",
    "    for i in range(len(df) - 1):  # Go up to second-to-last row\n",
    "        current_row = df.iloc[i]\n",
    "        next_row = df.iloc[i + 1]\n",
    "        \n",
    "        # Skip if current message is from assistant (creator)\n",
    "        if current_row['sender_handle'] == creator:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have a valid user-assistant pair\n",
    "        if (current_row['sender_handle'] != creator and \n",
    "            next_row['sender_handle'] == creator):\n",
    "            \n",
    "            pair = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": current_row['text']\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": next_row['text']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            jsonl_pairs.append(pair)\n",
    "    \n",
    "    return jsonl_pairs\n",
    "\n",
    "def save_to_jsonl(jsonl_pairs, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for pair in jsonl_pairs:\n",
    "            file.write(json.dumps(pair) + '\\n')\n",
    "\n",
    "# Process each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    # Load the training data\n",
    "    train_df = pd.read_csv(f'output_files/{creator}/train_concat_ordered_filtered.csv')\n",
    "    \n",
    "    # Generate the pairs and save to JSONL\n",
    "    pairs = generate_model_a_pairs(train_df, creator)\n",
    "    output_path = f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl'\n",
    "    save_to_jsonl(pairs, output_path)\n",
    "    \n",
    "    print(f\"Generated {len(pairs)} user-assistant pairs for {creator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the file with the lowest number of rows\n",
    "row_counts = {}\n",
    "for creator in dfs.keys():\n",
    "    jsonl_path = f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl'\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        rows = len(f.readlines())  # Count number of lines in JSONL file\n",
    "    row_counts[creator] = rows\n",
    "\n",
    "min_rows = min(row_counts.values())\n",
    "print(f\"\\nInitial row counts:\")\n",
    "for creator, count in row_counts.items():\n",
    "    print(f\"{creator}: {count} rows\")\n",
    "print(f\"\\nSmallest number of rows: {min_rows}\")\n",
    "\n",
    "# Save deletion statistics to file\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\n\\nStrategy-A JSONL Row Reduction Statistics:\\n\")\n",
    "    f.write(f\"Target row count (minimum): {min_rows}\\n\")\n",
    "    f.write(\"\\nRows removed per creator:\\n\")\n",
    "\n",
    "    # Equalize all files to the minimum number of rows\n",
    "    for creator in dfs.keys():\n",
    "        jsonl_path = f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl'\n",
    "        \n",
    "        # Read all rows\n",
    "        with open(jsonl_path, 'r') as file:\n",
    "            rows = [json.loads(line) for line in file]\n",
    "        \n",
    "        if len(rows) > min_rows:\n",
    "            rows_to_remove = len(rows) - min_rows\n",
    "            # Randomly select rows to keep\n",
    "            rows = random.sample(rows, min_rows)\n",
    "            \n",
    "            # Save back the reduced dataset\n",
    "            with open(jsonl_path, 'w') as file:\n",
    "                for row in rows:\n",
    "                    file.write(json.dumps(row) + '\\n')\n",
    "            \n",
    "            print(f\"\\n{creator}:\")\n",
    "            print(f\"- Original rows: {row_counts[creator]}\")\n",
    "            print(f\"- Rows removed: {rows_to_remove}\")\n",
    "            print(f\"- Final rows: {len(rows)}\")\n",
    "            \n",
    "            f.write(f\"\\n{creator}:\\n\")\n",
    "            f.write(f\"- Original rows: {row_counts[creator]}\\n\")\n",
    "            f.write(f\"- Rows removed: {rows_to_remove}\\n\")\n",
    "            f.write(f\"- Final rows: {len(rows)}\\n\")\n",
    "        else:\n",
    "            print(f\"\\n{creator}: No rows removed (already at minimum)\")\n",
    "            f.write(f\"\\n{creator}: No rows removed (already at minimum)\\n\")\n",
    "\n",
    "print(\"\\nRow reduction complete. All Strategy-A JSONL files now have the same number of rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model B: From 10 to 10 message pairs on row until that specific conversation ends. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Edge case 1: If a conversation has less than 20 messages - there will be only one chunk for that conversation.\n",
    "##### Edge case 2: The last chunk will have less than 20 if the number of total messages is not divisible by 20.\n",
    "##### Advantage: Has context.\n",
    "##### Disadvantage: Data is very compressed - will have fewer training rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conversations_model_b(df, creator, max_pairs=10):\n",
    "    jsonl_chunks = []\n",
    "    current_convo = []\n",
    "    current_user = None\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        user = row['recipient_handle'] if row['sender_handle'] == creator else row['sender_handle']\n",
    "        sender = row['sender_handle']\n",
    "        role = 'user' if sender != creator else 'assistant'\n",
    "        message = row['text']\n",
    "        \n",
    "        # Start new conversation if user changes\n",
    "        if user != current_user:\n",
    "            if current_convo:\n",
    "                # Process remaining messages in previous conversation\n",
    "                if len(current_convo) >= 2 and current_convo[-1]['role'] == 'assistant':\n",
    "                    while current_convo and current_convo[0]['role'] == 'assistant':\n",
    "                        current_convo = current_convo[1:]\n",
    "                    while current_convo and current_convo[-1]['role'] == 'user':\n",
    "                        current_convo = current_convo[:-1]\n",
    "                    if len(current_convo) >= 2:\n",
    "                        chunk = {\"messages\": current_convo}\n",
    "                        jsonl_chunks.append(chunk)\n",
    "            current_convo = []\n",
    "            current_user = user\n",
    "        \n",
    "        current_convo.append({'role': role, 'content': message})\n",
    "        \n",
    "        if len(current_convo) >= max_pairs * 2:\n",
    "            temp_convo = current_convo.copy()\n",
    "            while temp_convo and temp_convo[0]['role'] == 'assistant':\n",
    "                temp_convo = temp_convo[1:]\n",
    "            while temp_convo and temp_convo[-1]['role'] == 'user':\n",
    "                temp_convo = temp_convo[:-1]\n",
    "            if len(temp_convo) >= 2 and temp_convo[-1]['role'] == 'assistant':\n",
    "                chunk = {\"messages\": temp_convo}\n",
    "                jsonl_chunks.append(chunk)\n",
    "                current_convo = []\n",
    "    \n",
    "    if current_convo:\n",
    "        while current_convo and current_convo[0]['role'] == 'assistant':\n",
    "            current_convo = current_convo[1:]\n",
    "        while current_convo and current_convo[-1]['role'] == 'user':\n",
    "            current_convo = current_convo[:-1]\n",
    "        if len(current_convo) >= 2 and current_convo[-1]['role'] == 'assistant':\n",
    "            chunk = {\"messages\": current_convo}\n",
    "            jsonl_chunks.append(chunk)\n",
    "    \n",
    "    return jsonl_chunks\n",
    "\n",
    "# Process each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    # Load the training data\n",
    "    train_df = pd.read_csv(f'output_files/{creator}/train_concat_ordered_filtered.csv')\n",
    "    \n",
    "    # Generate the chunks and save to JSONL\n",
    "    chunks = process_conversations_model_b(train_df, creator)\n",
    "    output_path = f'output_files/{creator}/Strategy_B_ten_pairs_{creator}.jsonl'\n",
    "    save_to_jsonl(chunks, output_path)\n",
    "    \n",
    "    print(f\"Generated {len(chunks)} conversation chunks for {creator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the file with the lowest number of rows\n",
    "row_counts = {}\n",
    "for creator in dfs.keys():\n",
    "    jsonl_path = f'output_files/{creator}/Strategy_B_ten_pairs_{creator}.jsonl'\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        rows = len(f.readlines())  # Count number of lines in JSONL file\n",
    "    row_counts[creator] = rows\n",
    "\n",
    "min_rows = min(row_counts.values())\n",
    "print(f\"\\nInitial row counts:\")\n",
    "for creator, count in row_counts.items():\n",
    "    print(f\"{creator}: {count} rows\")\n",
    "print(f\"\\nSmallest number of rows: {min_rows}\")\n",
    "\n",
    "# Save deletion statistics to file\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\n\\nStrategy B JSONL Row Reduction Statistics:\\n\")\n",
    "    f.write(f\"Target row count (minimum): {min_rows}\\n\")\n",
    "    f.write(\"\\nRows removed per creator:\\n\")\n",
    "\n",
    "    # Equalize all files to the minimum number of rows\n",
    "    for creator in dfs.keys():\n",
    "        jsonl_path = f'output_files/{creator}/Strategy_B_ten_pairs_{creator}.jsonl'\n",
    "        \n",
    "        # Read all rows\n",
    "        with open(jsonl_path, 'r') as file:\n",
    "            rows = [json.loads(line) for line in file]\n",
    "        \n",
    "        if len(rows) > min_rows:\n",
    "            rows_to_remove = len(rows) - min_rows\n",
    "            # Randomly select rows to keep\n",
    "            rows = random.sample(rows, min_rows)\n",
    "            \n",
    "            # Save back the reduced dataset\n",
    "            with open(jsonl_path, 'w') as file:\n",
    "                for row in rows:\n",
    "                    file.write(json.dumps(row) + '\\n')\n",
    "            \n",
    "            print(f\"\\n{creator}:\")\n",
    "            print(f\"- Original rows: {row_counts[creator]}\")\n",
    "            print(f\"- Rows removed: {rows_to_remove}\")\n",
    "            print(f\"- Final rows: {len(rows)}\")\n",
    "            \n",
    "            f.write(f\"\\n{creator}:\\n\")\n",
    "            f.write(f\"- Original rows: {row_counts[creator]}\\n\")\n",
    "            f.write(f\"- Rows removed: {rows_to_remove}\\n\")\n",
    "            f.write(f\"- Final rows: {len(rows)}\\n\")\n",
    "        else:\n",
    "            print(f\"\\n{creator}: No rows removed (already at minimum)\")\n",
    "            f.write(f\"\\n{creator}: No rows removed (already at minimum)\\n\")\n",
    "\n",
    "print(\"\\nRow reduction complete. All Strategy B JSONL files now have the same number of rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C: Rolling format. Starts with one message pair and incrementally adds another pair from that specific conversation until it reaches 3 pairs. Then it rolls by incrementally removing the first pair and appending the next pair until there are no other message pairs in that conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantages: Simulates the natural flow of a conversation. Divesre training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def process_conversations(df, creator):\n",
    "    conversations = []\n",
    "    current_convo = []\n",
    "    current_user = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        user = row['recipient_handle'] if row['sender_handle'] == creator else row['sender_handle']\n",
    "        sender = row['sender_handle']\n",
    "        role = 'user' if sender != creator else 'assistant'\n",
    "        message = row['text']\n",
    "\n",
    "        if user != current_user:\n",
    "            if current_convo:\n",
    "                conversations.append(current_convo)\n",
    "            current_convo = []\n",
    "            current_user = user\n",
    "        \n",
    "        current_convo.append({'role': role, 'content': message})\n",
    "\n",
    "    if current_convo:\n",
    "        conversations.append(current_convo)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "def filter_conversation(convo):\n",
    "    if convo and convo[0]['role'] == 'assistant':\n",
    "        convo = convo[1:]\n",
    "    if convo and convo[-1]['role'] == 'user':\n",
    "        convo = convo[:-1]\n",
    "    return convo\n",
    "\n",
    "def generate_jsonl_chunks(conversation):\n",
    "    jsonl_chunks = []\n",
    "    start_index = 0\n",
    "    chunk_size = 2\n",
    "    max_pairs = 3  # Max pairs set to 3 (6 messages) - DUring preliminary tests it was even higher.\n",
    "\n",
    "    while start_index + chunk_size <= len(conversation):\n",
    "        chunk = conversation[start_index:start_index + chunk_size]\n",
    "        if len(chunk) % 2 == 0:  # Ensure even number of messages\n",
    "            jsonl_chunks.append({\"messages\": chunk})\n",
    "        chunk_size += 2\n",
    "        if chunk_size > max_pairs * 2:\n",
    "            start_index += 2\n",
    "            chunk_size = max_pairs * 2\n",
    "    \n",
    "    # Ensure the final chunk is the last X messages \n",
    "    remaining_chunk = conversation[-max_pairs * 2:]\n",
    "    if remaining_chunk and len(remaining_chunk) % 2 == 0 and \\\n",
    "       (not jsonl_chunks or remaining_chunk != jsonl_chunks[-1][\"messages\"]):\n",
    "        jsonl_chunks.append({\"messages\": remaining_chunk})\n",
    "\n",
    "    return jsonl_chunks\n",
    "\n",
    "def save_to_jsonl(jsonl_chunks, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for chunk in jsonl_chunks:\n",
    "            file.write(json.dumps(chunk) + '\\n')\n",
    "\n",
    "# Process each creator's dataset\n",
    "for creator, df in dfs.items():\n",
    "    print(f\"\\n=== Processing {creator}'s dataset ===\")\n",
    "    \n",
    "    # Read the training data\n",
    "    train_path = f'output_files/{creator}/train_concat_ordered_filtered.csv'\n",
    "    df = read_csv(train_path)\n",
    "    \n",
    "    # Process conversations\n",
    "    conversations = process_conversations(df, creator)\n",
    "    print(f\"Total conversations processed: {len(conversations)}\")\n",
    "\n",
    "    # Generate chunks\n",
    "    all_jsonl_chunks = []\n",
    "    for convo in conversations:\n",
    "        filtered_convo = filter_conversation(convo)\n",
    "        jsonl_chunks = generate_jsonl_chunks(filtered_convo)\n",
    "        all_jsonl_chunks.extend(jsonl_chunks)\n",
    "\n",
    "    # Save results\n",
    "    output_path = f'output_files/{creator}/Strategy_C_rolling_format_{creator}.jsonl'\n",
    "    save_to_jsonl(all_jsonl_chunks, output_path)\n",
    "    print(f\"Total chunks generated: {len(all_jsonl_chunks)}\")\n",
    "    print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the file with the lowest number of rows\n",
    "row_counts = {}\n",
    "for creator in dfs.keys():\n",
    "    jsonl_path = f'output_files/{creator}/Strategy_C_rolling_format_{creator}.jsonl'\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        rows = len(f.readlines())\n",
    "    row_counts[creator] = rows\n",
    "\n",
    "min_rows = min(row_counts.values())\n",
    "print(f\"\\nInitial row counts:\")\n",
    "for creator, count in row_counts.items():\n",
    "    print(f\"{creator}: {count} rows\")\n",
    "print(f\"\\nSmallest number of rows: {min_rows}\")\n",
    "\n",
    "# Save deletion statistics to file\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\n\\nStrategy C JSONL Row Reduction Statistics:\\n\")\n",
    "    f.write(f\"Target row count (minimum): {min_rows}\\n\")\n",
    "    f.write(\"\\nRows removed per creator:\\n\")\n",
    "\n",
    "    # Equalize all files to the minimum number of rows\n",
    "    for creator in dfs.keys():\n",
    "        jsonl_path = f'output_files/{creator}/Strategy_C_rolling_format_{creator}.jsonl'\n",
    "        \n",
    "        # Read all rows\n",
    "        with open(jsonl_path, 'r') as file:\n",
    "            rows = [json.loads(line) for line in file]\n",
    "        \n",
    "        if len(rows) > min_rows:\n",
    "            rows_to_remove = len(rows) - min_rows\n",
    "            # Randomly select rows to keep\n",
    "            rows = random.sample(rows, min_rows)\n",
    "            \n",
    "            # Save back the reduced dataset\n",
    "            with open(jsonl_path, 'w') as file:\n",
    "                for row in rows:\n",
    "                    file.write(json.dumps(row) + '\\n')\n",
    "            \n",
    "            print(f\"\\n{creator}:\")\n",
    "            print(f\"- Original rows: {row_counts[creator]}\")\n",
    "            print(f\"- Rows removed: {rows_to_remove}\")\n",
    "            print(f\"- Final rows: {len(rows)}\")\n",
    "            \n",
    "            f.write(f\"\\n{creator}:\\n\")\n",
    "            f.write(f\"- Original rows: {row_counts[creator]}\\n\")\n",
    "            f.write(f\"- Rows removed: {rows_to_remove}\\n\")\n",
    "            f.write(f\"- Final rows: {len(rows)}\\n\")\n",
    "        else:\n",
    "            print(f\"\\n{creator}: No rows removed (already at minimum)\")\n",
    "            f.write(f\"\\n{creator}: No rows removed (already at minimum)\\n\")\n",
    "\n",
    "print(\"\\nRow reduction complete. All Strategy C JSONL files now have the same number of rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Enhance the TESTING Data.\n",
    "By improving the format, we also anonymize the chat data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check real quick the min, average and max number of mesages per conversation in the tsting data. \n",
    "#### We will employ a similar format to the Model C/ Startegy C but with even more context. THis way we have no bias towards a given model. How? Well the test set will include rows with only one pair, with tons of pairs (and so context) and a few pairs but proper context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze test data for each creator\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\n\\nTest Set Conversation Statistics:\\n\")\n",
    "    \n",
    "    for creator in dfs.keys():\n",
    "        print(f\"\\n=== Test Set Analysis for {creator} ===\")\n",
    "        f.write(f\"\\n=== {creator} ===\\n\")\n",
    "        \n",
    "        # Load the test data\n",
    "        test_df = pd.read_csv(f'output_files/{creator}/test_concat_ordered_filtered.csv')\n",
    "        \n",
    "        # Get list of unique users (excluding creator)\n",
    "        unique_persons = set(test_df[test_df['sender_handle'] != creator]['sender_handle']) | \\\n",
    "                        set(test_df[test_df['recipient_handle'] != creator]['recipient_handle'])\n",
    "        \n",
    "        # Initialize dictionary to store conversation lengths\n",
    "        conversation_lengths = {}\n",
    "        \n",
    "        # Count messages in each conversation\n",
    "        for person in unique_persons:\n",
    "            conversation = test_df[\n",
    "                ((test_df['sender_handle'] == creator) & (test_df['recipient_handle'] == person)) |\n",
    "                ((test_df['sender_handle'] == person) & (test_df['recipient_handle'] == creator))\n",
    "            ]\n",
    "            conversation_lengths[person] = len(conversation)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        min_messages = min(conversation_lengths.values())\n",
    "        max_messages = max(conversation_lengths.values())\n",
    "        avg_messages = sum(conversation_lengths.values()) / len(conversation_lengths)\n",
    "        \n",
    "        # Print and save statistics\n",
    "        stats = [\n",
    "            f\"Number of unique persons in test set: {len(unique_persons)}\",\n",
    "            f\"\\nConversation Statistics:\",\n",
    "            f\"Minimum messages of a conversation: {min_messages}\",\n",
    "            f\"Average messages per conversation: {avg_messages:.2f}\",\n",
    "            f\"Maximum messages of a conversation: {max_messages}\"\n",
    "        ]\n",
    "        \n",
    "        for stat in stats:\n",
    "            print(stat)\n",
    "            f.write(stat + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conversations(df, creator):\n",
    "    conversations = []\n",
    "    current_convo = []\n",
    "    current_user = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        user = row['recipient_handle'] if row['sender_handle'] == creator else row['sender_handle']\n",
    "        sender = row['sender_handle']\n",
    "        role = 'user' if sender != creator else 'assistant'\n",
    "        message = row['text']\n",
    "\n",
    "        if user != current_user:\n",
    "            if current_convo:\n",
    "                conversations.append(current_convo)\n",
    "            current_convo = []\n",
    "            current_user = user\n",
    "        \n",
    "        current_convo.append({'role': role, 'content': message})\n",
    "\n",
    "    if current_convo:\n",
    "        conversations.append(current_convo)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "def filter_conversation(convo):\n",
    "    # Skip assistant messages at start\n",
    "    while convo and convo[0]['role'] == 'assistant':\n",
    "        convo = convo[1:]\n",
    "    # Skip user messages at end\n",
    "    while convo and convo[-1]['role'] == 'user':\n",
    "        convo = convo[:-1]\n",
    "    return convo\n",
    "\n",
    "def generate_test_jsonl_chunks(conversation):\n",
    "    jsonl_chunks = []\n",
    "    start_index = 0\n",
    "    chunk_size = 2\n",
    "    max_pairs = 6  # Set to 6 pairs (12 messages) for testing data\n",
    "\n",
    "    while start_index + chunk_size <= len(conversation):\n",
    "        chunk = conversation[start_index:start_index + chunk_size]\n",
    "        if len(chunk) % 2 == 0:  # Ensure even number of messages\n",
    "            jsonl_chunks.append({\"messages\": chunk})\n",
    "        chunk_size += 2\n",
    "        if chunk_size > max_pairs * 2:\n",
    "            start_index += 2\n",
    "            chunk_size = max_pairs * 2\n",
    "    \n",
    "    # Ensure the final chunk is the last 12 messages \n",
    "    remaining_chunk = conversation[-max_pairs * 2:]\n",
    "    if remaining_chunk and len(remaining_chunk) % 2 == 0 and \\\n",
    "       (not jsonl_chunks or remaining_chunk != jsonl_chunks[-1][\"messages\"]):\n",
    "        jsonl_chunks.append({\"messages\": remaining_chunk})\n",
    "\n",
    "    return jsonl_chunks\n",
    "\n",
    "# Process each creator's test dataset\n",
    "for creator in dfs.keys():\n",
    "    print(f\"\\n=== Processing Test Dataset for {creator} ===\")\n",
    "    \n",
    "    # Load the test data\n",
    "    test_df = pd.read_csv(f'output_files/{creator}/test_concat_ordered_filtered.csv')\n",
    "    \n",
    "    # Process conversations\n",
    "    conversations = process_conversations(test_df, creator)\n",
    "    \n",
    "    all_jsonl_chunks = []\n",
    "    for convo in conversations:\n",
    "        filtered_convo = filter_conversation(convo)\n",
    "        if filtered_convo:  # Only process if conversation is not empty after filtering\n",
    "            jsonl_chunks = generate_test_jsonl_chunks(filtered_convo)\n",
    "            all_jsonl_chunks.extend(jsonl_chunks)\n",
    "\n",
    "    # Save to JSONL file\n",
    "    output_path = f'output_files/{creator}/TEST_DATASET_{creator}.jsonl'\n",
    "    save_to_jsonl(all_jsonl_chunks, output_path)\n",
    "    \n",
    "    print(f\"Generated {len(all_jsonl_chunks)} test chunks for {creator}\")\n",
    "    print(f\"Test dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the file with the lowest number of test chunks\n",
    "row_counts = {}\n",
    "for creator in dfs.keys():\n",
    "    jsonl_path = f'output_files/{creator}/TEST_DATASET_{creator}.jsonl'\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        rows = len(f.readlines())\n",
    "    row_counts[creator] = rows\n",
    "\n",
    "min_rows = min(row_counts.values())\n",
    "print(f\"\\nInitial test chunk counts:\")\n",
    "for creator, count in row_counts.items():\n",
    "    print(f\"{creator}: {count} chunks\")\n",
    "print(f\"\\nSmallest number of test chunks: {min_rows}\")\n",
    "\n",
    "# Save deletion statistics to file\n",
    "with open('output_files/statistics.txt', 'a') as f:\n",
    "    f.write(\"\\n\\nTest Dataset Row Reduction Statistics:\\n\")\n",
    "    f.write(f\"Target chunk count (minimum): {min_rows}\\n\")\n",
    "    f.write(\"\\nChunks removed per creator:\\n\")\n",
    "\n",
    "    # Equalize all files to the minimum number of chunks\n",
    "    for creator in dfs.keys():\n",
    "        jsonl_path = f'output_files/{creator}/TEST_DATASET_{creator}.jsonl'\n",
    "        \n",
    "        # Read all chunks\n",
    "        with open(jsonl_path, 'r') as file:\n",
    "            chunks = [json.loads(line) for line in file]\n",
    "        \n",
    "        if len(chunks) > min_rows:\n",
    "            chunks_to_remove = len(chunks) - min_rows\n",
    "            # Randomly select chunks to keep\n",
    "            chunks = random.sample(chunks, min_rows)\n",
    "            \n",
    "            # Save back the reduced dataset\n",
    "            with open(jsonl_path, 'w') as file:\n",
    "                for chunk in chunks:\n",
    "                    file.write(json.dumps(chunk) + '\\n')\n",
    "            \n",
    "            print(f\"\\n{creator}:\")\n",
    "            print(f\"- Original chunks: {row_counts[creator]}\")\n",
    "            print(f\"- Chunks removed: {chunks_to_remove}\")\n",
    "            print(f\"- Final chunks: {len(chunks)}\")\n",
    "            \n",
    "            f.write(f\"\\n{creator}:\\n\")\n",
    "            f.write(f\"- Original chunks: {row_counts[creator]}\\n\")\n",
    "            f.write(f\"- Chunks removed: {chunks_to_remove}\\n\")\n",
    "            f.write(f\"- Final chunks: {len(chunks)}\\n\")\n",
    "        else:\n",
    "            print(f\"\\n{creator}: No chunks removed (already at minimum)\")\n",
    "            f.write(f\"\\n{creator}: No chunks removed (already at minimum)\\n\")\n",
    "\n",
    "print(\"\\nChunk reduction complete. All test datasets now have the same number of chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Add the same system message to all Json files.\n",
    "The models will train with a system message slightly steering them in the right directio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also adding a system message to the testing file. Why? When we are generating the responses, we are calling the model using the same specific system message tailored to each creator's details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each creator's files (including test files)\n",
    "for creator in dfs.keys():\n",
    "    system_message_content = f\" You are a content creator who goes by the name {creator}. You are assuming the creator's identity. \" # Example\n",
    "    \n",
    "    # List of all files including test dataset\n",
    "    all_files = [\n",
    "        f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl',\n",
    "        f'output_files/{creator}/Strategy_B_ten_pairs_{creator}.jsonl', \n",
    "        f'output_files/{creator}/Strategy_C_rolling_format_{creator}.jsonl',\n",
    "        f'output_files/{creator}/TEST_DATASET_{creator}.jsonl'  # Added test dataset\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n=== Processing {creator}'s files ===\")\n",
    "    \n",
    "    # Process each file\n",
    "    for jsonl_file in all_files:\n",
    "        print(f\"Processing {jsonl_file}...\")\n",
    "        \n",
    "        # Read the file\n",
    "        with open(jsonl_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Modify each line to include system message\n",
    "        modified_lines = []\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            # Prepend the system message if it's not already there\n",
    "            if not (data['messages'] and data['messages'][0].get('role') == 'system'):\n",
    "                system_message = {\"role\": \"system\", \"content\": system_message_content}\n",
    "                data['messages'].insert(0, system_message)\n",
    "            modified_lines.append(json.dumps(data))\n",
    "        \n",
    "        # Write back to the same file\n",
    "        with open(jsonl_file, 'w') as file:\n",
    "            for line in modified_lines:\n",
    "                file.write(line + '\\n')\n",
    "        \n",
    "        print(f\"Added system message to {jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Splitting the data into TRAIN - VAL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optional paramter while fine-tuning with OpenPipe is 'split' which can be asigned TRAIN or TEST. Like traditional ML task that use validation set to test the performance of their models, we visualy inspect the Model's output. We aknowledge that this is subjective and do not rely on it. First priority is to flag through the validation set if the Model 'behave' properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_split_labels(input_file, test_percentage=1):\n",
    "    # Read the JSONL file\n",
    "    with open(input_file, 'r') as file:\n",
    "        jsonl_data = [json.loads(line) for line in file]\n",
    "    \n",
    "    # Calculate number of test rows\n",
    "    total_rows = len(jsonl_data)\n",
    "    test_count = max(1, int(total_rows * (test_percentage / 100)))\n",
    "    \n",
    "    # Randomly select test indices\n",
    "    test_indices = set(random.sample(range(total_rows), test_count))\n",
    "    \n",
    "    # Add split labels\n",
    "    for i, entry in enumerate(jsonl_data):\n",
    "        entry['split'] = 'TEST' if i in test_indices else 'TRAIN'\n",
    "    \n",
    "    # Write back to the same file\n",
    "    with open(input_file, 'w') as file:\n",
    "        for entry in jsonl_data:\n",
    "            file.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    return total_rows, test_count\n",
    "\n",
    "# Process each creator's strategy files\n",
    "with open('output_files/statistics.txt', 'a') as stats_file:\n",
    "    stats_file.write(\"\\n\\nSplit Labels Statistics:\\n\")\n",
    "    \n",
    "    for creator in dfs.keys():\n",
    "        print(f\"\\n=== Processing {creator}'s Strategy Files ===\")\n",
    "        stats_file.write(f\"\\n=== {creator} ===\\n\")\n",
    "        \n",
    "        # List of strategy files for this creator\n",
    "        strategy_files = [\n",
    "            f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl',\n",
    "            f'output_files/{creator}/Strategy_B_ten_pairs_{creator}.jsonl',\n",
    "            f'output_files/{creator}/Strategy_C_rolling_format_{creator}.jsonl'\n",
    "        ]\n",
    "        \n",
    "        # Process each file\n",
    "        for file in strategy_files:\n",
    "            total_rows, test_count = add_split_labels(file)\n",
    "            \n",
    "            # Print and save statistics\n",
    "            stats = [\n",
    "                f\"Processed {file}:\",\n",
    "                f\"Total rows: {total_rows}\",\n",
    "                f\"Test rows: {test_count} (1%)\",\n",
    "                f\"Train rows: {total_rows - test_count} (99%)\\n\"\n",
    "            ]\n",
    "            \n",
    "            for stat in stats:\n",
    "                print(stat)\n",
    "                stats_file.write(stat + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL for other use-cases. The script below is optional and will divide the json files into 2 files (fine-tuneing with openpipe; can upload a file with maximum of 50k rows at a time). The first file will have 50k rows and the other the remaining. IN our case it doesn't apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_split_large_files(strategy_files, creator, row_limit=50000):\n",
    "    files_split = False\n",
    "    \n",
    "    for file_path in strategy_files:\n",
    "        # Read the JSONL file\n",
    "        with open(file_path, 'r') as file:\n",
    "            jsonl_data = [json.loads(line) for line in file]\n",
    "        \n",
    "        total_rows = len(jsonl_data)\n",
    "        \n",
    "        if total_rows > row_limit:\n",
    "            files_split = True\n",
    "            # Get file name components\n",
    "            strategy_name = file_path.split('_')[1]  # A, B, or C\n",
    "            \n",
    "            # Create file names for splits in creator's directory\n",
    "            first_file = f'output_files/{creator}/Strategy_{strategy_name}1_{creator}.jsonl'\n",
    "            second_file = f'output_files/{creator}/Strategy_{strategy_name}2_{creator}.jsonl'\n",
    "            \n",
    "            # Split the data\n",
    "            first_chunk = jsonl_data[:row_limit]\n",
    "            second_chunk = jsonl_data[row_limit:]\n",
    "            \n",
    "            # Save first chunk (up to 50k)\n",
    "            with open(first_file, 'w') as f:\n",
    "                for entry in first_chunk:\n",
    "                    f.write(json.dumps(entry) + '\\n')\n",
    "                    \n",
    "            # Save second chunk (remaining rows)\n",
    "            with open(second_file, 'w') as f:\n",
    "                for entry in second_chunk:\n",
    "                    f.write(json.dumps(entry) + '\\n')\n",
    "            \n",
    "            print(f\"Split {file_path}:\")\n",
    "            print(f\"- {first_file}: {len(first_chunk)} rows\")\n",
    "            print(f\"- {second_file}: {len(second_chunk)} rows\\n\")\n",
    "    \n",
    "    if not files_split:\n",
    "        print(f\"None of the files for {creator} need to split (all under 50k rows).\")\n",
    "\n",
    "# Process each creator's files\n",
    "for creator in dfs.keys():\n",
    "    print(f\"\\n=== Checking {creator}'s files for splitting ===\")\n",
    "    \n",
    "    # List of strategy files for this creator\n",
    "    strategy_files = [\n",
    "        f'output_files/{creator}/Strategy_A_one_pair_{creator}.jsonl',\n",
    "        f'output_files/{creator}/Strategy_B_ten_pairs_{creator}.jsonl',\n",
    "        f'output_files/{creator}/Strategy_C_rolling_format_{creator}.jsonl'\n",
    "    ]\n",
    "    \n",
    "    # Check and split files if needed\n",
    "    check_and_split_large_files(strategy_files, creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fine-tune the models - which in our case happens through OpenPipe (Amazing time and computational resources saver). This can be done through their Web Interface or API. Other methods of fine-tuning such as localy done or through hugging face API are possible and disscused/provided in the thesis.\n",
    "This part is sensitive but hyperparameter configuration will be shared. While the spotlight of this work relies on how the data is manipulated, consistnecy & reproducibility were part of the main focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible counter Questions:\n",
    "Q: How else would you test the models on the same thing? One cannot test the models on different testsets- it doesn't make any sense. And with the implemented method you have a bias-free dataset- a dataset that contains all the formats.\n",
    "\n",
    "Q: Maybe you would ask yourself but why dont you keep the initial amount of data, well because the only thing that should influence the performance of the Fine-tuned models should be the strategies that we have employed. We want to make sure that nothing esle might affect (too greatly) the final results.\n",
    "\n",
    "Q: Why did you trim the data in that point of the pipeline and not somewhere else? 1) where and how do you trim the data? In which point of the pipeline? 2) How do you maintain anyway the same amount of testing data in this case? You would ruin the flow and content of conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another debate : We could have more models for each iteration of improvmenetns (as we had initially) or we could have only 3 models but with much more testing data. Why? because of the resources it takes to train so many mdoels and also generate the reposnes. But fewer models to train means less money and winning some time, so that time can be fed back into generating responses at least."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
